{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "import pyldpc as ldpc\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools\n",
    "import scipy as sci\n",
    "import scipy.sparse as sp\n",
    "from scipy.stats import norm\n",
    "import scipy\n",
    "\n",
    "from utils_f import load_code\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "from IB_Decoder_Classes import symmetric_sIB\n",
    "from IB_Decoder_Classes import mutual_information as mutual_inf\n",
    "from IB_Decoder_Classes import lin_sym_sIB\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import pyopencl as cl\n",
    "    import pyopencl.array as cl_array\n",
    "    from pyopencl.clrandom import rand as clrand\n",
    "except ImportError:\n",
    "    Warning(\"PyOpenCl not installed\")\n",
    "import os\n",
    "from mako.template import Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWGN Channel Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWGN_channel:\n",
    "    \"\"\" Class implements an additive white Gaussian noise channel\n",
    "\n",
    "    The added noise can either be real or complex depending on the arguments of the constructor.\n",
    "    The default value is real.\n",
    "\n",
    "    Attributes:\n",
    "        sigma_n2: a double setting noise variance\n",
    "        complex: a boolean value indicating if noise is complex or not\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma_n2_, complex =False):\n",
    "        \"\"\"Inits the AWGN_channel class\n",
    "        Args:\n",
    "            sigma_n2_: noise variance specified by user\n",
    "            complex: default is false, indicating if noise is complex\n",
    "        \"\"\"\n",
    "        self.sigma_n2 = sigma_n2_\n",
    "        self.complex = complex\n",
    "\n",
    "    def transmission(self, input):\n",
    "        \"\"\"Performs the transmission of an input stream over an AWGN channel\n",
    "        Args:\n",
    "            input: sequence of symbols as numpy array or scalar\n",
    "        Returns:\n",
    "            output: summation of noise and input\n",
    "        \"\"\"\n",
    "        if self.complex:\n",
    "            noise = np.sqrt(self.sigma_n2/2) * np.random.randn(input.shape[0], input.shape[1]) + \\\n",
    "                    1j * np.sqrt(self.sigma_n2/2) * np.random.randn(input.shape[0], input.shape[1])\n",
    "\n",
    "        else:\n",
    "            noise = np.sqrt(self.sigma_n2) * np.random.randn(input.shape[0],input.shape[1])\n",
    "\n",
    "        output = input + noise\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWGN_Channel_Quantizer:\n",
    "    \"\"\"Implementation of an information optimum quantizer unit assuming BPSK transmission.\n",
    "\n",
    "    The quantizer is generated using the symmetric, sequential information bottleneck algorithm.\n",
    "    This class supports OpenCL for faster quantization and even direct quantization and sample generation on the GPU\n",
    "    (cf. quantize direct).\n",
    "    Although it is theoretical correct to quantize directly, it is preferable to create a more realistic\n",
    "    communication chain including an encoder and modulator in your system instead of using this direct quantization approach.\n",
    "\n",
    "    Attributes:\n",
    "        sigma_n2: noise variance corresponding to the desired design-Eb/N0 of the decoder\n",
    "        AD_max_abs: limits of the quantizer\n",
    "        cardinality_Y: number of steps used for the fine quantization of the input distribution of the quantizer\n",
    "        cardinality_T: cardinality of the compression variable representing the quantizer output\n",
    "\n",
    "        limits: borders of the quantizer regions\n",
    "        y_vec: fine quantization of the input domain\n",
    "        delta: spacing between two values in the quantized input domain (cf. y_vec)\n",
    "\n",
    "        x_vec: position of the means of the involved Gaussians\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma_n2_, AD_max_abs_, cardinality_T_, cardinality_Y_, dont_calc = False):\n",
    "        \"\"\"Inits the quantizer class.\"\"\"\n",
    "        self.nror = 5\n",
    "        self.limits = np.zeros(cardinality_T_)\n",
    "\n",
    "        self.sigma_n2 = sigma_n2_\n",
    "        self.cardinality_T = cardinality_T_\n",
    "        self.cardinality_Y = cardinality_Y_\n",
    "        self.AD_max_abs = AD_max_abs_\n",
    "\n",
    "        self.y_vec = np.linspace(-self.AD_max_abs, +self.AD_max_abs, self.cardinality_Y)\n",
    "        self.x_vec = np.array([-1, 1])\n",
    "        self.delta = self.y_vec[1] - self.y_vec[0]\n",
    "        if not dont_calc:\n",
    "            self.calc_quanti()\n",
    "\n",
    "    def calc_quanti(self):\n",
    "        \"\"\"Determines the information optimum quantizer for the given input distribution\"\"\"\n",
    "\n",
    "        # calculate p_xy based on sigma_n2 and AD_max_abs;\n",
    "        # init as normal with mean + 1\n",
    "        p_y_given_x_equals_zero = norm.pdf(self.y_vec, loc=1, scale=np.sqrt(self.sigma_n2)) * self.delta\n",
    "\n",
    "        # truncate t account for distortion introduced by the quantizer limits then\n",
    "        p_y_given_x_equals_zero[-1] += self.gaussian_over_prob(self.AD_max_abs, 1)\n",
    "        p_y_given_x_equals_zero[0] += self.gaussian_under_prob(-self.AD_max_abs, 1)\n",
    "\n",
    "        # flip distribution, which realizes mean -1 or a transmitted bit = 1\n",
    "        p_y_given_x_equals_one = p_y_given_x_equals_zero[::-1]\n",
    "\n",
    "        self.p_xy = 0.5 * np.hstack((p_y_given_x_equals_zero[:,np.newaxis], p_y_given_x_equals_one[:,np.newaxis]))\n",
    "\n",
    "        self.p_xy = self.p_xy / self.p_xy.sum() #normalize for munerical stability\n",
    "\n",
    "        # run the symmetric sequential Information Bottleneck algorithm\n",
    "        IB_class = symmetric_sIB(self.p_xy, self.cardinality_T, self.nror)\n",
    "        IB_class.run_IB_algo()\n",
    "\n",
    "        # store the results\n",
    "        [self.p_t_given_y, self.p_x_given_t, self.p_t] = IB_class.get_results()\n",
    "\n",
    "        # calculate\n",
    "        # p(t | X = 0)=p(X=0 | t)\n",
    "        # p(t) / p(X=0)\n",
    "        self.p_x_given_t = self.p_x_given_t / self.p_x_given_t.sum(1)[:,np.newaxis]\n",
    "        self.p_x_and_t = self.p_x_given_t * self.p_t[:,np.newaxis]\n",
    "        p_t_given_x_equals_zero = self.p_x_and_t[:, 0] / 0.5\n",
    "\n",
    "        self.cdf_t_given_x_equals_zero = np.append([0], np.cumsum(p_t_given_x_equals_zero))\n",
    "\n",
    "        self.output_LLRs = np.log(self.p_x_and_t[:, 0] / self.p_x_and_t[:, 1])\n",
    "        self.calc_limits()\n",
    "\n",
    "    @classmethod\n",
    "    def from_generated(cls, cdf_t_given_x_equals_zero_):\n",
    "        cdf_t_given_x_equals_zero = cdf_t_given_x_equals_zero_\n",
    "        return cls(cdf_t_given_x_equals_zero,)\n",
    "\n",
    "    def gaussian_over_prob(self, x, mu):\n",
    "        \"\"\"Compensates the ignored probability mass caused by fixing the region to +- AD_abs_max.\"\"\"\n",
    "\n",
    "        prob = norm.sf((x-mu+self.delta/2)/np.sqrt(self.sigma_n2))\n",
    "        return prob\n",
    "\n",
    "    def gaussian_under_prob(self, x, mu):\n",
    "        \"\"\"Compensates the ignored probability mass caused by fixing the region to +- AD_abs_max.\"\"\"\n",
    "\n",
    "        prob = 1-self.gaussian_over_prob(x-self.delta,mu)\n",
    "        return prob\n",
    "\n",
    "    def calc_limits(self):\n",
    "        \"\"\"Calculates the limits of the quantizer borders\"\"\"\n",
    "\n",
    "        for i in range(self.cardinality_T):\n",
    "            cur_vec = (self.p_t_given_y[:, i] == 1).nonzero()\n",
    "            self.limits[i] = self.y_vec[cur_vec[0].min()]\n",
    "\n",
    "        self.limits[int(self.cardinality_T/2)] = 0\n",
    "        #self.limits[-1]=self.AD_max_abs\n",
    "\n",
    "    def quantize_direct(self, input_bits):\n",
    "        \"\"\"Direct quantization without the need of a channel in between since the inversion method is used.\n",
    "        The clusters are directly sampled.\n",
    "        \"\"\"\n",
    "        # create uniform samples\n",
    "        rand_u = np.random.rand(input_bits.shape[0],input_bits.shape[1])\n",
    "\n",
    "        # create samples ~ p(t | X = 0) using inversion method\n",
    "        if input_bits.shape[1] > 1:\n",
    "            output_integers = ((np.repeat(rand_u[:,:,np.newaxis], self.cardinality_T+1, axis=2)-self.cdf_t_given_x_equals_zero) > 0).sum(2)-1\n",
    "            output_integers[input_bits.astype(bool)] = self.cardinality_T - 1 - output_integers[input_bits.astype(bool)]\n",
    "        else:\n",
    "            output_integers = ((rand_u - self.cdf_t_given_x_equals_zero) > 0).sum(1) - 1\n",
    "            # \"mirror\" a sample, when the input bit is 1, otherwise do nothing.\n",
    "            output_integers[input_bits.astype(bool)[:, 0]] = self.cardinality_T - 1 - output_integers[\n",
    "            input_bits.astype(bool)[:, 0]]\n",
    "\n",
    "        return output_integers\n",
    "\n",
    "    def quantize_on_host(self,x):\n",
    "        \"\"\"Quantizes the received samples on the local machine\"\"\"\n",
    "        if x.shape[1] > 1:\n",
    "            cluster = ((np.repeat(x[:,:,np.newaxis], self.cardinality_T, axis=2)-self.limits) > 0).sum(2)-1\n",
    "            cluster[cluster == -1] = 0\n",
    "        else:\n",
    "            cluster = np.sum((x - self.limits) > 0, 1) -1\n",
    "            cluster[cluster==-1] = 0\n",
    "\n",
    "        return cluster\n",
    "\n",
    "    def init_OpenCL_quanti(self, N_var,msg_at_time,return_buffer_only=False):\n",
    "        \"\"\"Inits the OpenCL context and transfers all static data to the device\"\"\"\n",
    "\n",
    "        self.context = cl.create_some_context()\n",
    "\n",
    "        print(self.context.get_info(cl.context_info.DEVICES))\n",
    "        path = os.path.split(os.path.abspath(\"__file__\"))\n",
    "        kernelsource = open(os.path.join(path[0], 'kernels_quanti_template.cl')).read()\n",
    "\n",
    "        tpl = Template(kernelsource)\n",
    "        rendered_tp = tpl.render(Nvar=N_var)\n",
    "\n",
    "        self.program = cl.Program(self.context, str(rendered_tp)).build()\n",
    "\n",
    "        self.return_buffer_only = return_buffer_only\n",
    "\n",
    "        # Set up OpenCL\n",
    "        self.queue = cl.CommandQueue(self.context)\n",
    "        self.quantize = self.program.quantize\n",
    "        self.quantize.set_scalar_arg_dtypes([np.int32, None, None, None])\n",
    "        self.quantize_LLR = self.program.quantize_LLR\n",
    "        self.quantize_LLR.set_scalar_arg_dtypes([np.int32, None, None, None,None])\n",
    "        self.limit_buff = cl_array.to_device(self.queue, self.cdf_t_given_x_equals_zero.astype(np.float64))\n",
    "        self.cluster_buff = cl_array.empty(self.queue, (N_var, msg_at_time), dtype=np.int32)\n",
    "        self.LLR_buff = cl_array.empty(self.queue, (N_var, msg_at_time), dtype=np.float64)\n",
    "        self.LLR_values_buff = cl_array.to_device(self.queue, self.output_LLRs.astype(np.float64))\n",
    "\n",
    "    def quantize_OpenCL(self, x):\n",
    "        \"\"\"Quantizes the received distorted samples on the graphic card\"\"\"\n",
    "\n",
    "        # Create OpenCL buffers\n",
    "\n",
    "        x_buff = cl_array.to_device(self.queue,x.astype(np.float64) )\n",
    "        limit_buff = cl_array.to_device(self.queue, self.limits.astype(np.float64))\n",
    "        cluster_buff = cl_array.empty_like(x_buff.astype(np.int32))\n",
    "\n",
    "        self.quantize(self.queue, x.shape, None, self.cardinality_T, x_buff.data, limit_buff.data, cluster_buff.data)\n",
    "        self.queue.finish()\n",
    "\n",
    "        if self.return_buffer_only:\n",
    "            return cluster_buff\n",
    "        else:\n",
    "            clusters = cluster_buff.get()\n",
    "            return clusters\n",
    "\n",
    "    def quantize_direct_OpenCL(self,N_var,msg_at_time):\n",
    "        \"\"\"Direct quantization without the need of a channel in between since the inversion method is used.\n",
    "        The clusters are directly sampled. In this scenario the all-zeros codeword is considered such that no data\n",
    "        needs to be transferred to the graphic card.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        #rand_u_buff = clrand(self.queue, (N_var,msg_at_time), dtype=np.float64, a=0, b=1)\n",
    "\n",
    "        rand_u = np.random.rand(N_var,msg_at_time)\n",
    "\n",
    "        # Create OpenCL buffers\n",
    "\n",
    "        rand_u_buff = cl_array.to_device(self.queue,rand_u.astype(np.float64) )\n",
    "\n",
    "\n",
    "\n",
    "        self.quantize(self.queue, (N_var,msg_at_time), None, self.cardinality_T+1, rand_u_buff.data,\n",
    "                      self.limit_buff.data, self.cluster_buff.data)\n",
    "\n",
    "\n",
    "        self.queue.finish()\n",
    "\n",
    "        if self.return_buffer_only:\n",
    "            return self.cluster_buff\n",
    "        else:\n",
    "            clusters = self.cluster_buff.get()\n",
    "            return clusters\n",
    "\n",
    "    def quantize_direct_OpenCL_LLR(self,N_var,msg_at_time):\n",
    "        \"\"\" Returns the LLRs of the sampled cluster indices. These indices correspond to the quantized outputs which\n",
    "        are found directly on the graphic card using the inversion method. \"\"\"\n",
    "\n",
    "        rand_u = np.random.rand(N_var,msg_at_time)\n",
    "\n",
    "        # Create OpenCL buffers\n",
    "        rand_u_buff = cl_array.to_device(self.queue,rand_u.astype(np.float64) )\n",
    "\n",
    "        self.quantize_LLR(self.queue, (N_var,msg_at_time), None, self.cardinality_T+1, rand_u_buff.data,\n",
    "                      self.limit_buff.data, self.LLR_values_buff.data, self.LLR_buff.data)\n",
    "\n",
    "        self.queue.finish()\n",
    "\n",
    "        if self.return_buffer_only:\n",
    "            return self.LLR_buff\n",
    "        else:\n",
    "            LLRs = self.LLR_buff.get()\n",
    "            return LLRs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDPC Transmitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDPC_BPSK_Transmitter:\n",
    "\n",
    "    def __init__(self, filename_H_,  msg_at_time=1):\n",
    "        self.filename_H = filename_H_\n",
    "        self.H_sparse = self.load_check_mat(self.filename_H)\n",
    "\n",
    "        self.encoder = LDPCEncoder(self.filename_H)\n",
    "\n",
    "        # analyze the H matrix and set all decoder variables\n",
    "        self.set_code_parameters()\n",
    "\n",
    "        self.data_len = (self.R_c * self.codeword_len).astype(int)\n",
    "\n",
    "        self.last_transmitted_bits = []\n",
    "        self.msg_at_time = msg_at_time\n",
    "\n",
    "    def set_code_parameters(self):\n",
    "        self.degree_checknode_nr = ((self.H_sparse).sum(1)).astype(np.int).A[:, 0]  # which check node has which degree?\n",
    "        self.degree_varnode_nr = ((self.H_sparse).sum(0)).astype(np.int).A[0,\n",
    "                                 :]  # which variable node has which degree?\n",
    "\n",
    "        self.N_v = self.H_sparse.shape[1]  # How many variable nodes are present?\n",
    "        self.N_c = self.H_sparse.shape[0]  # How many checknodes are present?\n",
    "\n",
    "        self.d_c_max = self.degree_checknode_nr.max()\n",
    "        self.d_v_max = self.degree_varnode_nr.max()\n",
    "\n",
    "        self.codeword_len = self.H_sparse.shape[1]\n",
    "        row_sum = self.H_sparse.sum(0).A[0, :]\n",
    "        col_sum = self.H_sparse.sum(1).A[:, 0]\n",
    "        d_v_dist_val = np.unique(row_sum)\n",
    "        d_v_dist = np.zeros(int(d_v_dist_val.max()))\n",
    "\n",
    "        for d_v in np.sort(d_v_dist_val).astype(np.int):\n",
    "            d_v_dist[d_v - 1] = (row_sum == d_v).sum()\n",
    "        d_v_dist = d_v_dist / d_v_dist.sum()\n",
    "\n",
    "        d_c_dist_val = np.unique(col_sum)\n",
    "        d_c_dist = np.zeros(int(d_c_dist_val.max()))\n",
    "\n",
    "        for d_c in np.sort(d_c_dist_val).astype(np.int):\n",
    "            d_c_dist[d_c - 1] = (col_sum == d_c).sum()\n",
    "\n",
    "        d_c_dist = d_c_dist / d_c_dist.sum()\n",
    "        nom = np.dot(d_v_dist, np.arange(d_v_dist_val.max()) + 1)\n",
    "        den = np.dot(d_c_dist, np.arange(d_c_dist_val.max()) + 1)\n",
    "\n",
    "        self.R_c = 1 - nom / den\n",
    "\n",
    "    def alistToNumpy(self, lines):\n",
    "        \"\"\"Converts a parity-check matrix in AList format to a 0/1 numpy array. The argument is a\n",
    "       list-of-lists corresponding to the lines of the AList format, already parsed to integers\n",
    "        if read from a text file.\n",
    "        The AList format is introduced on http://www.inference.phy.cam.ac.uk/mackay/codes/alist.html.\n",
    "        This method supports a \"reduced\" AList format where lines 3 and 4 (containing column and row\n",
    "        weights, respectively) and the row-based information (last part of the Alist file) are omitted.\n",
    "        Example:\n",
    "             >>> alistToNumpy([[3,2], [2, 2], [1,1,2], [2,2], [1], [2], [1,2], [1,2,3,4]])\n",
    "            array([[1, 0, 1],\n",
    "                  [0, 1, 1]])\n",
    "        \"\"\"\n",
    "\n",
    "        nCols, nRows = lines[0]\n",
    "        if len(lines[2]) == nCols and len(lines[3]) == nRows:\n",
    "            startIndex = 4\n",
    "        else:\n",
    "            startIndex = 2\n",
    "        matrix = np.zeros((nRows, nCols), dtype=np.int)\n",
    "        for col, nonzeros in enumerate(lines[startIndex:startIndex + nCols]):\n",
    "            for rowIndex in nonzeros:\n",
    "                if rowIndex != 0:\n",
    "                    matrix[rowIndex - 1, col] = 1\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    def load_sparse_csr(self, filename):\n",
    "        loader = np.load(filename)\n",
    "        return sci.sparse.csr_matrix((loader['data'], loader['indices'], loader['indptr']),\n",
    "                              shape=loader['shape'])\n",
    "\n",
    "    def load_check_mat(self, filename):\n",
    "        if filename.endswith('.npy') or filename.endswith('.npz'):\n",
    "            if filename.endswith('.npy'):\n",
    "                H = np.load(filename)\n",
    "                H_sparse = sci.sparse.csr_matrix(H)\n",
    "            else:\n",
    "                H_sparse = self.load_sparse_csr(filename)\n",
    "        else:\n",
    "            arrays = [np.array(list(map(int, line.split()))) for line in open(filename)]\n",
    "            H = self.alistToNumpy(arrays)\n",
    "            H_sparse = sci.sparse.csr_matrix(H)\n",
    "        return H_sparse\n",
    "\n",
    "    def transmit(self):\n",
    "\n",
    "        uncoded_msgs = np.random.randint(0,2, (self.data_len, self.msg_at_time))\n",
    "\n",
    "        #uncoded_msgs = np.zeros( (self.data_len, self.msg_at_time) )\n",
    "        encoded_msgs = np.zeros((self.codeword_len, self.msg_at_time))\n",
    "\n",
    "\n",
    "        for i in range(self.msg_at_time):\n",
    "\n",
    "            encoded_msgs[:, i]=self.encoder.encode_c(uncoded_msgs[:, i])\n",
    "\n",
    "        self.last_transmitted_bits = uncoded_msgs\n",
    "\n",
    "        data = self.BPSK_mapping(encoded_msgs)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def BPSK_mapping(self, X):\n",
    "\n",
    "        data = np.ones((self.codeword_len, self.msg_at_time))\n",
    "        data[X == 1] = -1\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete density evolution class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discrete_Density_Evolution_class:\n",
    "    \"\"\"This class enforces symmetric Trellis diagram by using the lin_sym_sIB class. See the documentation for further\n",
    "    information about this Information Bottleneck algorithm\n",
    "    Attributes:\n",
    "    input parameter\n",
    "        p_x_and_t_input             initial pdf p(x,t)\n",
    "        cardinality_T_decoder_ops   message alphabet cardinality of the passed decoder messages\n",
    "        d_v                         variable node degree\n",
    "        d_c                         check node degree\n",
    "        i_max                       number of iterations\n",
    "    IB related parameters\n",
    "        cardinality_T               number of clusters\n",
    "        beta                        set to inf, due to deterministic mapping\n",
    "        eps\n",
    "        nror                        Number Of Runs, namely number of runs with different initial clusterings\n",
    "    discrete DE parameters\n",
    "\n",
    "    \"\"\"\n",
    "    PROBABILITY_MIN_JOINT_PDF = 1e-15\n",
    "    PROBABILITY_MAX_JOINT_PDF = 0.5-1e-15\n",
    "\n",
    "    def __init__(self, p_x_and_t_input_, cardinality_T_decoder_ops_, d_v_, d_c_, i_max_, nror_):\n",
    "        # initialize parameters\n",
    "        self.p_x_and_t_channel_init = p_x_and_t_input_\n",
    "        #determine cardinalities\n",
    "        self.cardinality_T_channel = self.p_x_and_t_channel_init.shape[0]\n",
    "        self.cardinality_T_decoder_ops = cardinality_T_decoder_ops_\n",
    "\n",
    "        # code related parameters\n",
    "        self.d_v = d_v_\n",
    "        self.d_c = d_c_\n",
    "\n",
    "        # discrete DE paramters\n",
    "        self.i_max = i_max_\n",
    "\n",
    "        # IB parameters\n",
    "        self.nror = nror_\n",
    "\n",
    "        # generate look up tables (LUT) for all input combinations. Basically this is a linear remapping from a 2D space\n",
    "        # onto a 1D vector. Due to the possible mismatch cardinality_T_channel != cardinality_T_decoder_ops, a specific\n",
    "        # loop up table is generated for this case, since the number of possible input combination of a partial node\n",
    "        # operation could vary.\n",
    "\n",
    "\n",
    "        ###### ------ CHECK NODES Preallocation START ------ ######\n",
    "        # As explained earlier due to the possible mismatch of cardinality_T_channel and cardinality_T_decoder_ops the\n",
    "        # first iteration has to be treated separately. In this first iteration the first partial node operation at the\n",
    "        # check node we have incoming messages from the channel taking cardinality_T_channel different values. The next\n",
    "        # partial node operation gets an input vector containing the result of the first partial node operation being\n",
    "        # cardinality cardinality_T_decoder_ops and a second input value from a variable node with cardinality_T_channel.\n",
    "        # In the next iteration all incoming and outgoing messages can take only cardinality_T_decoder_ops values.\n",
    "\n",
    "        # Preallocate LUTs\n",
    "\n",
    "        # Case iter = 0\n",
    "        self.all_two_input_combinations_first_partial_op_check_first_iter = np.hstack((\n",
    "        np.kron(np.arange(self.cardinality_T_channel)[:, np.newaxis], np.ones([self.cardinality_T_channel, 1])),\n",
    "        np.tile(np.arange(self.cardinality_T_channel)[:, np.newaxis], (self.cardinality_T_channel, 1)) ))\n",
    "\n",
    "        # Case iter = 0, channel and cardinality_T_decoder_ops input\n",
    "        self.all_two_input_combinations_other_partial_ops_check_first_iter = np.hstack((\n",
    "        np.kron(np.arange(self.cardinality_T_decoder_ops)[:, np.newaxis], np.ones([self.cardinality_T_channel, 1])),\n",
    "        np.tile(np.arange(self.cardinality_T_channel)[:, np.newaxis], (self.cardinality_T_decoder_ops, 1))))\n",
    "\n",
    "        # Case iter >0, only cardinality_T_decoder_ops\n",
    "        self.all_two_input_combinations_other_partial_ops_check = np.hstack((\n",
    "        np.kron(np.arange(self.cardinality_T_decoder_ops)[:, np.newaxis], np.ones([self.cardinality_T_decoder_ops, 1])),\n",
    "        np.tile(np.arange(self.cardinality_T_decoder_ops)[:, np.newaxis], (self.cardinality_T_decoder_ops, 1))))\n",
    "\n",
    "        self.p_t_l_c_given_vec_y_l_c_collection = np.empty((self.i_max, self.d_c-2), dtype=object)\n",
    "        self.p_x_l_c_given_t_l_c_collection = np.empty((self.i_max, self.d_c-2), dtype=object)\n",
    "        self.sorted_vec_y_l_c_collection = np.empty((self.i_max, self.d_c-2), dtype=object)\n",
    "        self.p_t_l_c_collection = np.empty((self.i_max, self.d_c-2), dtype=object)\n",
    "\n",
    "\n",
    "        self.Trellis_checknodevector_a = \\\n",
    "            np.zeros(self.cardinality_T_channel**2 \\\n",
    "                     + (self.d_c-3)*self.cardinality_T_decoder_ops*self.cardinality_T_channel \\\n",
    "                     + (self.i_max-1)*self.cardinality_T_decoder_ops**2*(self.d_c-2))\n",
    "\n",
    "        ###### ------ CHECK NODES Preallocation End ------ ######\n",
    "\n",
    "        ###### ------ VARIABLE NODES Preallocation Start ------ ######\n",
    "\n",
    "        # First message at the variable nodes is the quantized channel output. Consequently, this message can take\n",
    "        # cardinality_T_channel values. The other incomming message are from check nodes and therefore are limited to\n",
    "        # cardinality_T_decoder_ops values.\n",
    "\n",
    "        self.all_two_input_combinations_first_partial_ops_var = np.hstack((\n",
    "            np.kron(np.arange(self.cardinality_T_channel)[:, np.newaxis], np.ones([self.cardinality_T_decoder_ops, 1])),\n",
    "            np.tile(np.arange(self.cardinality_T_decoder_ops)[:, np.newaxis], (self.cardinality_T_channel, 1))))\n",
    "\n",
    "\n",
    "        self.all_two_input_combinations_other_partial_ops_var = np.hstack((\n",
    "            np.kron(np.arange(self.cardinality_T_decoder_ops)[:, np.newaxis], np.ones([self.cardinality_T_decoder_ops, 1])),\n",
    "            np.tile(np.arange(self.cardinality_T_decoder_ops)[:, np.newaxis], (self.cardinality_T_decoder_ops, 1))))\n",
    "\n",
    "        self.p_t_l_v_given_vec_y_l_v_collection = np.empty((self.i_max, self.d_v), dtype=object)\n",
    "        self.p_x_l_v_given_t_l_v_collection = np.empty((self.i_max, self.d_v), dtype=object)\n",
    "        self.sorted_vec_y_l_v_collection = np.empty((self.i_max, self.d_v), dtype=object)\n",
    "        self.p_t_l_v_collection = np.empty((self.i_max, self.d_v), dtype=object)\n",
    "\n",
    "\n",
    "        self.Trellis_varnodevector_a = \\\n",
    "            np.zeros( (self.i_max) * (self.cardinality_T_channel * self.cardinality_T_decoder_ops\n",
    "                                      + (self.d_v - 1) * self.cardinality_T_decoder_ops ** 2))\n",
    "\n",
    "        ###### ------ VARIABLE NODES Preallocation END ------ ######\n",
    "\n",
    "        # preallocate vectors for mutual information results\n",
    "        self.MI_T_dvm1_v_X_dvm1_v = np.zeros(self.i_max)\n",
    "        self.MI_Y_dvm1_v_X_dvm1_v = np.zeros(self.i_max)\n",
    "        self.mutual_inf_gain_matrix = np.zeros((self.i_max,self.d_v) )\n",
    "\n",
    "    def check_node_density_evolution(self, iteration, p_x_lminus1_c_and_t_lminus1_c, p_b_lplus1_c_and_y_lplus1_c):\n",
    "        print(\"check_node_density_evolution\"+str(iteration))\n",
    "        for w in range(self.d_c - 2):\n",
    "            p_x_l_c_and_vec_y_l_c_lin = self.checknode_in_joint_pdf_y_lin(p_x_lminus1_c_and_t_lminus1_c,\n",
    "                                                                          p_b_lplus1_c_and_y_lplus1_c)\n",
    "\n",
    "            # run Information Bottleneck algorithm on p_x_l_c_and_vec_y_l_c_lin\n",
    "            IB_instance = lin_sym_sIB(self.numerical_quard(p_x_l_c_and_vec_y_l_c_lin),\n",
    "                                      self.cardinality_T_decoder_ops,\n",
    "                                      self.nror)\n",
    "\n",
    "            IB_instance.run_IB_algo()\n",
    "\n",
    "            # get results and feed back\n",
    "            p_t_l_c_given_vec_y_l_c, p_x_lminus1_c_given_t_lminus1_c, p_t_lm1_c = IB_instance.get_results()\n",
    "            p_x_lminus1_c_and_t_lminus1_c = p_x_lminus1_c_given_t_lminus1_c * p_t_lm1_c[:,np.newaxis]\n",
    "\n",
    "\n",
    "            IB_instance.display_MIs(short=True)\n",
    "\n",
    "            # save message mappings for the check nodes explicitly in  cell array\n",
    "            self.p_t_l_c_given_vec_y_l_c_collection[iteration, w] = p_t_l_c_given_vec_y_l_c\n",
    "            self.p_x_l_c_given_t_l_c_collection[iteration, w] = p_x_lminus1_c_given_t_lminus1_c\n",
    "            self.p_t_l_c_collection[iteration, w] = p_t_lm1_c\n",
    "\n",
    "            if iteration == 0 and w == 0:\n",
    "                self.sorted_vec_y_l_c_collection[iteration, w] = \\\n",
    "                    self.all_two_input_combinations_first_partial_op_check_first_iter\n",
    "            if iteration == 0 and w > 0:\n",
    "                self.sorted_vec_y_l_c_collection[iteration, w] = \\\n",
    "                    self.all_two_input_combinations_other_partial_ops_check_first_iter\n",
    "            if iteration > 0:\n",
    "\n",
    "                self.sorted_vec_y_l_c_collection[iteration, w] = self.all_two_input_combinations_other_partial_ops_check\n",
    "\n",
    "        return p_x_lminus1_c_and_t_lminus1_c\n",
    "\n",
    "    def variable_node_density_evolution(self, iteration, p_x_lminus1_c_and_t_lminus1_c):\n",
    "        print(\"variable_node_density_evolution number\"+str(iteration))\n",
    "\n",
    "        # The first message is from the channel and the second message is a message from the other nodes per definition\n",
    "        p_b_lplus1_v_and_y_lplus1_v = self.p_x_and_t_channel_init # channel\n",
    "        p_x_lminus1_v_and_t_lminus1_v = p_x_lminus1_c_and_t_lminus1_c # output of DE for the check nodes\n",
    "\n",
    "        p_x_l_v_and_vec_y_l_v_lin = self.varnode_in_joint_pdf_y_lin(p_b_lplus1_v_and_y_lplus1_v,\n",
    "                                                                    p_x_lminus1_v_and_t_lminus1_v)\n",
    "\n",
    "\n",
    "        # run Information Bottleneck algorithm on p_x_l_v_and_vec_y_l_v_lin\n",
    "        IB_instance = lin_sym_sIB(self.numerical_quard(p_x_l_v_and_vec_y_l_v_lin),\n",
    "                                  self.cardinality_T_decoder_ops,\n",
    "                                  self.nror)\n",
    "\n",
    "        IB_instance.run_IB_algo()\n",
    "\n",
    "        # get results and feed back\n",
    "        p_t_l_v_given_vec_y_l_v, p_x_lminus1_v_given_t_lminus1_v, p_t_lm1_v = IB_instance.get_results()\n",
    "\n",
    "        p_x_lminus1_v_and_t_lminus1_v = p_x_lminus1_v_given_t_lminus1_v * p_t_lm1_v[:, np.newaxis]\n",
    "\n",
    "        IB_instance.display_MIs(short=True)\n",
    "        self.mutual_inf_gain_matrix[iteration,0] = IB_instance.get_mutual_inf()[0]\n",
    "\n",
    "        # save message mappings for the variable nodes explicitly in  cell array\n",
    "        self.p_t_l_v_given_vec_y_l_v_collection[iteration, 0] = p_t_l_v_given_vec_y_l_v\n",
    "        self.p_x_l_v_given_t_l_v_collection[iteration, 0] = p_x_lminus1_v_given_t_lminus1_v\n",
    "        self.p_t_l_v_collection[iteration, 0] = p_t_lm1_v\n",
    "\n",
    "\n",
    "        self.sorted_vec_y_l_v_collection[iteration, 0] = self.all_two_input_combinations_first_partial_ops_var\n",
    "\n",
    "        # the rest of the message pdfs are all from check nodes\n",
    "        p_b_lplus1_v_and_y_lplus1_v = p_x_lminus1_c_and_t_lminus1_c\n",
    "\n",
    "        for w in range(1, self.d_v - 1):\n",
    "            p_x_l_v_and_vec_y_l_v_lin = self.varnode_in_joint_pdf_y_lin(p_x_lminus1_v_and_t_lminus1_v,\n",
    "                                                                        p_b_lplus1_v_and_y_lplus1_v)\n",
    "\n",
    "            # run Information Bottleneck algorithm on p_x_l_v_and_vec_y_l_v_lin\n",
    "            IB_instance = lin_sym_sIB(self.numerical_quard(p_x_l_v_and_vec_y_l_v_lin),\n",
    "                                      self.cardinality_T_decoder_ops,\n",
    "                                      self.nror)\n",
    "\n",
    "            IB_instance.run_IB_algo()\n",
    "\n",
    "            # get results and feed back\n",
    "            p_t_l_v_given_vec_y_l_v, p_x_lminus1_v_given_t_lminus1_v, p_t_lm1_v = IB_instance.get_results()\n",
    "\n",
    "            p_x_lminus1_v_and_t_lminus1_v = p_x_lminus1_v_given_t_lminus1_v * p_t_lm1_v[:, np.newaxis]\n",
    "\n",
    "            IB_instance.display_MIs(short=True)\n",
    "            self.mutual_inf_gain_matrix[iteration,w] = IB_instance.get_mutual_inf()[0] - self.mutual_inf_gain_matrix[iteration,:].sum()\n",
    "\n",
    "            # save message mappings for the variable nodes explicitly in  cell array\n",
    "            self.p_t_l_v_given_vec_y_l_v_collection[iteration, w] = p_t_l_v_given_vec_y_l_v\n",
    "            self.p_x_l_v_given_t_l_v_collection[iteration, w] = p_x_lminus1_v_given_t_lminus1_v\n",
    "            self.p_t_l_v_collection[iteration, w] = p_t_lm1_v\n",
    "\n",
    "            self.sorted_vec_y_l_v_collection[iteration, w] = self.all_two_input_combinations_other_partial_ops_var\n",
    "\n",
    "        de_varnode_out = p_x_lminus1_v_and_t_lminus1_v / p_x_lminus1_v_and_t_lminus1_v.sum()\n",
    "\n",
    "        # and run the IB algorithm for the last time to create the message mappings of the decision mapping for the\n",
    "        # variable nodes\n",
    "        p_x_l_v_and_vec_y_l_v_lin = self.varnode_in_joint_pdf_y_lin(p_x_lminus1_v_and_t_lminus1_v,\n",
    "                                                                  p_b_lplus1_v_and_y_lplus1_v)\n",
    "\n",
    "\n",
    "        IB_instance = lin_sym_sIB(self.numerical_quard(p_x_l_v_and_vec_y_l_v_lin),\n",
    "                                  self.cardinality_T_decoder_ops,\n",
    "                                  self.nror)\n",
    "\n",
    "        IB_instance.run_IB_algo()\n",
    "\n",
    "        # get results and save\n",
    "        p_t_l_v_given_vec_y_l_v, p_x_lminus1_v_given_t_lminus1_v, p_t_lm1_v = IB_instance.get_results()\n",
    "\n",
    "        IB_instance.display_MIs(short=True)\n",
    "        self.mutual_inf_gain_matrix[iteration,-1] = IB_instance.get_mutual_inf()[0] - \\\n",
    "                                                    self.mutual_inf_gain_matrix[iteration,:].sum()\n",
    "        # and save the resulting message mapping\n",
    "        self.p_t_l_v_given_vec_y_l_v_collection[iteration, -1] = p_t_l_v_given_vec_y_l_v\n",
    "        self.p_x_l_v_given_t_l_v_collection[iteration, -1] = p_x_lminus1_v_given_t_lminus1_v\n",
    "        self.p_t_l_v_collection[iteration, -1] = p_t_lm1_v\n",
    "\n",
    "        # finally store I(T_(d_v - 1} ^ v, X{d_v - 1} ^ v)\n",
    "        self.MI_T_dvm1_v_X_dvm1_v[iteration], self.MI_Y_dvm1_v_X_dvm1_v[iteration] = IB_instance.get_mutual_inf()\n",
    "\n",
    "        self.sorted_vec_y_l_v_collection[iteration, -1] = self.all_two_input_combinations_other_partial_ops_var\n",
    "\n",
    "        return de_varnode_out\n",
    "\n",
    "    def run_discrete_Density_Evolution(self):\n",
    "        \"\"\"This function combines all subroutines to implement discrete density evolution\"\"\"\n",
    "\n",
    "        #  in the first iteration all incoming messages are directly from the channel\n",
    "        p_x_lminus1_c_and_t_lminus1_c = self.p_x_and_t_channel_init / self.p_x_and_t_channel_init.sum()\n",
    "        p_b_lplus1_c_and_y_lplus1_c = self.p_x_and_t_channel_init / self.p_x_and_t_channel_init.sum()\n",
    "\n",
    "        # create theIB_calc object(init values are arbitrary, in density evolution loop, the member function init()\n",
    "        # is called to set up the object properly, but we do not have a standard constructor in this class )\n",
    "\n",
    "        self.ext_mi_varnode_in_iter = np.empty(self.i_max+1)\n",
    "        self.ext_mi_checknode_in_iter = np.empty(self.i_max)\n",
    "\n",
    "        self.ext_mi_varnode_in_iter[0] = mutual_inf(self.p_x_and_t_channel_init)\n",
    "\n",
    "        for i in range(self.i_max):\n",
    "\n",
    "            # check node density evolution\n",
    "            de_checknode_out = self.check_node_density_evolution(i, p_x_lminus1_c_and_t_lminus1_c, p_b_lplus1_c_and_y_lplus1_c)\n",
    "            self.ext_mi_checknode_in_iter[i] = mutual_inf(de_checknode_out)\n",
    "\n",
    "            # variable node density evolution\n",
    "            de_varnode_out = self.variable_node_density_evolution(i, de_checknode_out)\n",
    "            self.ext_mi_varnode_in_iter[i+1] = mutual_inf(de_varnode_out)\n",
    "\n",
    "            # set feed back variables to the varnode density evolution\n",
    "            # normalize for stability\n",
    "            p_b_lplus1_c_and_y_lplus1_c = de_varnode_out\n",
    "            p_x_lminus1_c_and_t_lminus1_c = de_varnode_out\n",
    "\n",
    "\n",
    "        # calculate the Trellis vector vec_a for the checknodes for all partial steps in the first decoder iteration\n",
    "        # first partial node operation:\n",
    "        # @param iteration: denotes the iteration\n",
    "        # @param l: indicates teh partial node operation index\n",
    "\n",
    "        l=0\n",
    "        self.Trellis_checknodevector_a[:(l+1)*self.cardinality_T_channel**2] = \\\n",
    "            np.argmax(self.p_t_l_c_given_vec_y_l_c_collection[0, l], axis=1).astype(int)\n",
    "\n",
    "        # and now for the other partial node operations of the first iteration, where l>0\n",
    "        for l in range(1, self.d_c-2):\n",
    "            # the offset marks the jump in address for this iteration\n",
    "            offset = 1*self.cardinality_T_channel**2+(l-1)*self.cardinality_T_channel*self.cardinality_T_decoder_ops\n",
    "\n",
    "            # length denotes the number new samples, which are filled in Trellis_Trellis_checknodevector_a\n",
    "            length = self.cardinality_T_channel * self.cardinality_T_decoder_ops\n",
    "\n",
    "            self.Trellis_checknodevector_a[offset:offset+length] = \\\n",
    "                np.argmax(self.p_t_l_c_given_vec_y_l_c_collection[0, l], axis=1).astype(int)\n",
    "\n",
    "\n",
    "        # calc for iterations > 0\n",
    "        initial_offset = 1*self.cardinality_T_channel**2+(self.d_c-3)*self.cardinality_T_channel*self.cardinality_T_decoder_ops\n",
    "        for iteration in range(1, self.i_max):\n",
    "            iter_offset = (iteration-1) * (self.d_c-2) * self.cardinality_T_decoder_ops**2\n",
    "            for l in range(self.d_c - 2):\n",
    "                offset = initial_offset + iter_offset + l*self.cardinality_T_decoder_ops**2\n",
    "                length = self.cardinality_T_decoder_ops**2\n",
    "\n",
    "                self.Trellis_checknodevector_a[offset:offset + length] = \\\n",
    "                    np.argmax(self.p_t_l_c_given_vec_y_l_c_collection[iteration, l], axis=1).astype(int)\n",
    "\n",
    "\n",
    "        # Trellis vector for variable nodes\n",
    "        for iteration in range(self.i_max):\n",
    "            l = 0\n",
    "            iter_offset = iteration * (self.cardinality_T_channel * self.cardinality_T_decoder_ops +\n",
    "                                      (self.d_v - 1) * self.cardinality_T_decoder_ops ** 2)\n",
    "\n",
    "            length = self.p_t_l_v_given_vec_y_l_v_collection[iteration, l].shape[0]\n",
    "\n",
    "            self.Trellis_varnodevector_a[iter_offset:iter_offset + length] = \\\n",
    "                np.argmax(self.p_t_l_v_given_vec_y_l_v_collection[iteration, l], axis=1).astype(int)\n",
    "\n",
    "            offset_after_first_op = iter_offset+length\n",
    "\n",
    "            for l in range(1, self.d_v):\n",
    "                offset = offset_after_first_op + (l - 1) * (self.cardinality_T_decoder_ops ** 2)\n",
    "                length = self.cardinality_T_decoder_ops ** 2\n",
    "                self.Trellis_varnodevector_a[offset:offset+length] = \\\n",
    "                    np.argmax(self.p_t_l_v_given_vec_y_l_v_collection[iteration, l], axis=1).astype(int)\n",
    "\n",
    "    def checknode_in_joint_pdf_y_lin(self, p_t_lminus1_c_and_x_lminus1_c, p_y_lplus1_c_and_b_lplus1_c ):\n",
    "        \"\"\" This function calculates the joint pdf p(vec_y_l ^ c, x_l ^ c) that is needed for the density evolution. The\n",
    "        result is  a matrix. Each row corresponds to a linearized vector vec_y_l ^ c\n",
    "\n",
    "        Please note that according to the\n",
    "        paper the vector vec_y_l ^ c = [t_{l - 1} ^ c, y_{l + 1} ^ c] and that the order is extremely important, because\n",
    "        a linear coordinate is calculated from this vector by the rule\n",
    "        vec_y_lin = | \\mathcal {Y} ^ c | t_ {l - 1} ^ c + y_  {l + 1} ^ c\n",
    "        Args:\n",
    "            :param p_t_lminus1_c_and_x_lminus1_c:   the density p(t_lm1_c,x_lm1_c)\n",
    "            :param p_y_lplus1_c_and_b_lplus1_c:   the density p(y_lp1_c,b_lp1_c)\n",
    "        Return\n",
    "            :return p_x_c_and_y_vec_c:\n",
    "        \"\"\"\n",
    "\n",
    "        # determine the cardinality of the first input. Referring to the paper this is the message entering from the top\n",
    "        cardinality_Y_i_first_in = p_t_lminus1_c_and_x_lminus1_c.shape[0]\n",
    "        cardinality_Y_i_second_in = p_y_lplus1_c_and_b_lplus1_c.shape[0]\n",
    "        cardinality_vec_y_l = cardinality_Y_i_first_in*cardinality_Y_i_second_in\n",
    "\n",
    "        p_x_c_and_y_vec_c = np.zeros((cardinality_vec_y_l, 2))\n",
    "        p_x_c_and_y_vec_c2 = np.zeros((cardinality_vec_y_l, 2))\n",
    "\n",
    "        # In the following the sum is evaluated. There exist two case where x^c can be 0 or 1. Furthermore the \\oplus\n",
    "        # sum of b_0 and b_1 results in each of the previously mentioned cases, on two different ways. These are denoted\n",
    "        # as part 1 and part 2. The sum of these parts equals the result for either x^c=0 or 1.\n",
    "\n",
    "        y_lp1_vec = np.kron(np.arange(cardinality_Y_i_second_in)[:, np.newaxis], np.ones([cardinality_Y_i_first_in, 1])).astype(int)\n",
    "        t_lm1_vec = np.tile(np.arange(cardinality_Y_i_first_in)[:, np.newaxis], (cardinality_Y_i_second_in,1))\n",
    "\n",
    "        # case x ^ c = 0, where x ^ c = b_0 ^ c \\oplus b_1 ^ c\n",
    "        # case b_0 ^ c = 0 b_1 ^ c = 0 ,case b_0 ^ c = 1 b_1 ^ c = 1\n",
    "        part10 = p_y_lplus1_c_and_b_lplus1_c[y_lp1_vec, :] * p_t_lminus1_c_and_x_lminus1_c[t_lm1_vec, :]\n",
    "\n",
    "        p_x_c_and_y_vec_c[cardinality_Y_i_second_in * t_lm1_vec + y_lp1_vec, 0] = part10.sum(2) #+ part20\n",
    "\n",
    "        # case x ^ c = 1, where x ^ c = b_0 ^ c \\oplus b_1 ^ c\n",
    "        # case b_0 ^ c = 0 b_1 ^ c = 1 , case b_0 ^ c = 1  b_1 ^ c = 0\n",
    "        part11 = p_y_lplus1_c_and_b_lplus1_c[y_lp1_vec, :] * p_t_lminus1_c_and_x_lminus1_c[t_lm1_vec, ::-1]\n",
    "\n",
    "        p_x_c_and_y_vec_c[cardinality_Y_i_second_in * t_lm1_vec + y_lp1_vec, 1] = part11.sum(2)\n",
    "\n",
    "        return p_x_c_and_y_vec_c\n",
    "\n",
    "    def varnode_in_joint_pdf_y_lin(self, p_t_lminus1_v_and_x_lminus1_v, p_y_lplus1_v_and_b_lplus1_v):\n",
    "        \"\"\" This function calculates the joint pdf p(vec_y_l ^ v, x_l ^ v) that is needed for the density evolution. The\n",
    "        result is  a matrix. Each row corresponds to a linearized vector vec_y_l ^ v\n",
    "\n",
    "        Please note that according to the\n",
    "        paper the vector vec_y_l ^ c = [t_{l - 1} ^ v, y_{l + 1} ^ v] and that the order is extremely important, because\n",
    "        a linear coordinate is calculated from this vector by the rule\n",
    "        vec_y_lin = | \\mathcal {Y} ^ v | t_ {l - 1} ^ v + y_  {l + 1} ^ v\n",
    "        Args:\n",
    "            :param p_t_lminus1_v_and_x_lminus1_v:   the density p(t_lm1_v,x_lm1_v)\n",
    "            :param p_y_lplus1_v_and_b_lplus1_v:   the density p(y_lp1_v,b_lp1_v)\n",
    "        Return\n",
    "            :return p_x_v_and_y_vec_v:\n",
    "        \"\"\"\n",
    "\n",
    "        # determine the cardinality of the first input. Referring to the paper this is the message entering from the top\n",
    "        cardinality_Y_i_first_in = p_t_lminus1_v_and_x_lminus1_v.shape[0]\n",
    "        cardinality_Y_i_second_in = p_y_lplus1_v_and_b_lplus1_v.shape[0]\n",
    "        cardinality_vec_y_l = cardinality_Y_i_first_in * cardinality_Y_i_second_in\n",
    "\n",
    "        p_x_v_and_y_vec_v = np.zeros((cardinality_vec_y_l, 2))\n",
    "\n",
    "        # In the following the sum is evaluated. There exist two case where x^c can be 0 or 1. Furthermore the \\oplus\n",
    "        # sum of b_0 and b_1 results in each of the previously mentioned cases, on two different ways. These are denoted\n",
    "        # as part 1 and part 2. The sum of these parts equals the result for either x^c=0 or 1.\n",
    "\n",
    "        y_lp1_vec = np.kron(np.arange(cardinality_Y_i_second_in)[:, np.newaxis],\n",
    "                            np.ones([cardinality_Y_i_first_in, 1])).astype(int)\n",
    "        t_lm1_vec = np.tile(np.arange(cardinality_Y_i_first_in)[:, np.newaxis], (cardinality_Y_i_second_in, 1))\n",
    "\n",
    "        # case x ^ c = 0, where x ^ c = b_0 ^ c \\oplus b_1 ^ c\n",
    "        # case b_0 ^ c = 0 b_1 ^ c = 0 ,case b_0 ^ c = 1 b_1 ^ c = 1\n",
    "        part10 = 2 * p_y_lplus1_v_and_b_lplus1_v[y_lp1_vec, 0] * p_t_lminus1_v_and_x_lminus1_v[t_lm1_vec, 0]\n",
    "\n",
    "        p_x_v_and_y_vec_v[cardinality_Y_i_second_in * t_lm1_vec + y_lp1_vec, 0] = part10\n",
    "\n",
    "        # case x ^ c = 1, where x ^ c = b_0 ^ c \\oplus b_1 ^ c\n",
    "        # case b_0 ^ c = 0 b_1 ^ c = 1 , case b_0 ^ c = 1  b_1 ^ c = 0\n",
    "        part11 = 2 * p_y_lplus1_v_and_b_lplus1_v[y_lp1_vec, 1] * p_t_lminus1_v_and_x_lminus1_v[t_lm1_vec, 1]\n",
    "\n",
    "        p_x_v_and_y_vec_v[cardinality_Y_i_second_in * t_lm1_vec + y_lp1_vec, 1] = part11\n",
    "\n",
    "        return p_x_v_and_y_vec_v\n",
    "\n",
    "    def numerical_quard(self, pdf):\n",
    "        \"\"\"Function to avoid numerical instabilities.\"\"\"\n",
    "        limited_pdf = pdf\n",
    "        limited_pdf[limited_pdf <= self.PROBABILITY_MIN_JOINT_PDF] = self.PROBABILITY_MIN_JOINT_PDF\n",
    "        limited_pdf[limited_pdf >= self.PROBABILITY_MAX_JOINT_PDF] = self.PROBABILITY_MAX_JOINT_PDF\n",
    "        limited_pdf = limited_pdf / limited_pdf.sum()\n",
    "        return limited_pdf\n",
    "\n",
    "    def visualize_mi_evolution(self):\n",
    "        plt.plot(self.MI_T_dvm1_v_X_dvm1_v)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWGN Discrete Density Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWGN_Discrete_Density_Evolution_class:\n",
    "    \"\"\" Generates a discrete LDPC decoder for a AWGN channel and a regular LDPC code for a certain design-Eb/N0.\n",
    "\n",
    "    The assumed modulation is BPSK which is considered in the quantizer design.\n",
    "    Attributes:\n",
    "        sigma_n2: noise variance corresponding to the desired design-Eb/N0 of the decoder\n",
    "        AD_max_abs: limits of the quantizer\n",
    "        cardinality_Y_channel: number of steps used for the fine quantization of the input distribution of the quantizer\n",
    "        cardinality_T_channel: cardinality of the compression variable representing the quantizer output\n",
    "\n",
    "        cardinality_T_decoder_ops: cardinality of the compression variables inside the decoder\n",
    "\n",
    "        d_c: check node degree\n",
    "        d_v: variable node degree\n",
    "\n",
    "        imax: maximum number of iterations\n",
    "        nror: number of runs of the Information Bottleneck algorithm\n",
    "\n",
    "        Trellis_checknodevector_a:  vectorized version of the trellis which holds the resulting outputs for a certain\n",
    "                                    input and iteration at a check node\n",
    "        Trellis_varnodevector_a:  vectorized version of the trellis which holds the resulting outputs for a certain\n",
    "                                    input and iteration at a variable node\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma_n2_, AD_max_abs_,cardinality_Y_channel_, cardinality_T_channel_,\n",
    "                 cardinality_T_decoder_ops_,d_v_, d_c_, i_max_, nror_):\n",
    "        \"\"\"Inits the AWGN_Discrete_Density_Evolution_class with the following arguments\n",
    "\n",
    "        Args:\n",
    "            sigma_n2_: noise variance corresponding to the desired design-Eb/N0 of the decoder\n",
    "            AD_max_abs_: limits of the quantizer\n",
    "            cardinality_Y_channel_: number of steps used for the fine quantization of the input distribution of the quantizer\n",
    "            cardinality_T_channel_: cardinality of the compression variable representing the quantizer output\n",
    "\n",
    "            cardinality_T_decoder_ops_: cardinality of the compression variables inside the decoder\n",
    "\n",
    "            d_c_: check node degree\n",
    "            d_v_: variable node degree\n",
    "\n",
    "            i_max_: maximum number of iterations\n",
    "            nror_: number of runs of the Information Bottleneck algorithm\n",
    "        \"\"\"\n",
    "        # copy input arguments to class attributes\n",
    "        self.sigma_n2 = sigma_n2_\n",
    "        self.AD_max_abs = AD_max_abs_\n",
    "\n",
    "        self.cardinality_Y_channel = cardinality_Y_channel_\n",
    "        self.cardinality_T_channel = cardinality_T_channel_\n",
    "        self.cardinality_T_decoder_ops = cardinality_T_decoder_ops_\n",
    "\n",
    "        self.d_v = d_v_\n",
    "        self.d_c = d_c_\n",
    "\n",
    "        R_c = 1 - self.d_v / self.d_c\n",
    "        if R_c > 0:\n",
    "            self.EbN0 = -10 * np.log10(self.sigma_n2 * 2 * R_c)\n",
    "\n",
    "        self.imax = i_max_\n",
    "        self.nror = nror_\n",
    "\n",
    "        self.build_quantizer()\n",
    "\n",
    "\n",
    "        self.Trellis_checknodevector_a = 0\n",
    "        self.Trellis_varnodevector_a = 0\n",
    "\n",
    "    def set_code_parameters(self):\n",
    "        \"\"\"Analysis of the given parity check matrix.\n",
    "        Determines node-degree distribution, edge-degree distribution and code rate\n",
    "        \"\"\"\n",
    "        self.degree_checknode_nr = ((self.H_sparse).sum(1)).astype(np.int).A[:, 0]  # which check node has which degree?\n",
    "        self.degree_varnode_nr = ((self.H_sparse).sum(0)).astype(np.int).A[0,\n",
    "                                 :]  # which variable node has which degree?\n",
    "\n",
    "        self.N_v = self.H_sparse.shape[1]  # How many variable nodes are present?\n",
    "        self.N_c = self.H_sparse.shape[0]  # How many checknodes are present?\n",
    "\n",
    "        self.d_c_max = self.degree_checknode_nr.max()\n",
    "        self.d_v_max = self.degree_varnode_nr.max()\n",
    "\n",
    "        self.codeword_len = self.H_sparse.shape[1]\n",
    "        row_sum = self.H_sparse.sum(0).A[0, :]\n",
    "        col_sum = self.H_sparse.sum(1).A[:, 0]\n",
    "        d_v_dist_val = np.unique(row_sum)\n",
    "        d_v_dist = np.zeros(int(d_v_dist_val.max()))\n",
    "\n",
    "        for d_v in np.sort(d_v_dist_val).astype(np.int):\n",
    "            d_v_dist[d_v - 1] = (row_sum == d_v).sum()\n",
    "\n",
    "        d_v_dist = d_v_dist / d_v_dist.sum()\n",
    "\n",
    "        d_c_dist_val = np.unique(col_sum)\n",
    "        d_c_dist = np.zeros(int(d_c_dist_val.max()))\n",
    "\n",
    "        for d_c in np.sort(d_c_dist_val).astype(np.int):\n",
    "            d_c_dist[d_c - 1] = (col_sum == d_c).sum()\n",
    "\n",
    "        d_c_dist = d_c_dist / d_c_dist.sum()\n",
    "\n",
    "        nom = np.dot(d_v_dist, np.arange(d_v_dist_val.max()) + 1)\n",
    "        den = np.dot(d_c_dist, np.arange(d_c_dist_val.max()) + 1)\n",
    "\n",
    "        self.lambda_vec = convert_node_to_edge_degree(d_v_dist)\n",
    "        self.rho_vec = convert_node_to_edge_degree(d_c_dist)\n",
    "\n",
    "        self.R_c = 1 - nom / den\n",
    "\n",
    "    def alistToNumpy(self, lines):\n",
    "        \"\"\"Converts a parity-check matrix in AList format to a 0/1 numpy array. The argument is a\n",
    "        list-of-lists corresponding to the lines of the AList format, already parsed to integers\n",
    "        if read from a text file.\n",
    "        The AList format is introduced on http://www.inference.phy.cam.ac.uk/mackay/codes/alist.html.\n",
    "        This method supports a \"reduced\" AList format where lines 3 and 4 (containing column and row\n",
    "        weights, respectively) and the row-based information (last part of the Alist file) are omitted.\n",
    "        Example:\n",
    "             >>> alistToNumpy([[3,2], [2, 2], [1,1,2], [2,2], [1], [2], [1,2], [1,2,3,4]])\n",
    "            array([[1, 0, 1],\n",
    "                  [0, 1, 1]])\n",
    "        \"\"\"\n",
    "\n",
    "        nCols, nRows = lines[0]\n",
    "        if len(lines[2]) == nCols and len(lines[3]) == nRows:\n",
    "            startIndex = 4\n",
    "        else:\n",
    "            startIndex = 2\n",
    "        matrix = np.zeros((nRows, nCols), dtype=np.int)\n",
    "        for col, nonzeros in enumerate(lines[startIndex:startIndex + nCols]):\n",
    "            for rowIndex in nonzeros:\n",
    "                if rowIndex != 0:\n",
    "                    matrix[rowIndex - 1, col] = 1\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    def load_sparse_csr(self, filename):\n",
    "        \"\"\"Performs loading of a sparse parity check matrix which is stored in a *.npy file.\"\"\"\n",
    "        loader = np.load(filename)\n",
    "        return sci.sparse.csr_matrix((loader['data'], loader['indices'], loader['indptr']),\n",
    "                                     shape=loader['shape'])\n",
    "\n",
    "    def load_check_mat(self, filename):\n",
    "        \"\"\"Performs loading of a predefined parity check matrix.\"\"\"\n",
    "        if filename.endswith('.npy') or filename.endswith('.npz'):\n",
    "            if filename.endswith('.npy'):\n",
    "                H = np.load(filename)\n",
    "                H_sparse = sci.sparse.csr_matrix(H)\n",
    "            else:\n",
    "                H_sparse = self.load_sparse_csr(filename)\n",
    "        else:\n",
    "            arrays = [np.array(list(map(int, line.split()))) for line in open(filename)]\n",
    "            H = self.alistToNumpy(arrays)\n",
    "            H_sparse = sci.sparse.csr_matrix(H)\n",
    "        return H_sparse\n",
    "\n",
    "    def build_quantizer(self):\n",
    "        \"\"\"Generates instance of a quantizer for BPSK and an AWGN channel for the given characteristics.\"\"\"\n",
    "        quanti = AWGN_Channel_Quantizer(self.sigma_n2,self.AD_max_abs,self.cardinality_T_channel,self.cardinality_Y_channel)\n",
    "        self.p_x_and_t_input = quanti.p_x_and_t\n",
    "\n",
    "    def run_discrete_density_evolution(self):\n",
    "        \"\"\"Performs the discrete density evolution using the input distributions obtained from the quantizer.\n",
    "        The resulting trellis diagram is stored in a vector that can be used for the real decoder later.\n",
    "        \"\"\"\n",
    "        DDE_inst = Discrete_Density_Evolution_class(self.p_x_and_t_input, self.cardinality_T_decoder_ops,\n",
    "                               self.d_v, self.d_c, self.imax, self.nror)\n",
    "\n",
    "        DDE_inst.run_discrete_Density_Evolution()\n",
    "\n",
    "        self.Trellis_checknodevector_a = DDE_inst.Trellis_checknodevector_a\n",
    "        self.Trellis_varnodevector_a = DDE_inst.Trellis_varnodevector_a\n",
    "\n",
    "        self.DDE_inst_data = DDE_inst.__dict__\n",
    "\n",
    "    def save_config(self,text=''):\n",
    "        \"\"\"Saves the instance.\"\"\"\n",
    "        #timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        timestr =''\n",
    "\n",
    "        output = open('decoder_config_EbN0_gen_' + str(self.EbN0) + '_' + str(\n",
    "            self.cardinality_T_decoder_ops) + timestr + text + '.pkl', 'wb')\n",
    "\n",
    "        # Pickle dictionary using protocol -1.\n",
    "        pickle.dump(self.__dict__, output, protocol=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create decoder for Regular LDPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EbN0_dB_mapping_gen = 1.2\n",
    "\n",
    "\n",
    "# set quantizer limits\n",
    "AD_Max_abs = 3\n",
    "\n",
    "\n",
    "cardinality_Y_channel = 2000\n",
    "cardinality_T_channel = 16\n",
    "cardinality_T_decoder_ops = 16\n",
    "i_max = 200\n",
    "nror = 4\n",
    "\n",
    "# set code related parameters\n",
    "d_v = 3\n",
    "d_c = 6\n",
    "\n",
    "R_c = 1-d_v/d_c    # code rate\n",
    "\n",
    "sigma_n2 = 10**(-EbN0_dB_mapping_gen/10) / (2*R_c)\n",
    "\n",
    "# generate decoder config\n",
    "DDE_inst = AWGN_Discrete_Density_Evolution_class(sigma_n2, AD_Max_abs, cardinality_Y_channel, cardinality_T_channel,\n",
    "               cardinality_T_decoder_ops, d_v, d_c, i_max, nror )\n",
    "DDE_inst.run_discrete_density_evolution()\n",
    "\n",
    "\n",
    "# generate trajectory\n",
    "\n",
    "x_vec = np.zeros(2*i_max-1)\n",
    "y_vec = np.zeros(2*i_max-1)\n",
    "\n",
    "x_vec[0] = 0\n",
    "y_vec[0] = DDE_inst.DDE_inst_data['ext_mi_varnode_in_iter'][0]\n",
    "\n",
    "\n",
    "for i in range(1,i_max):\n",
    "    x_vec[2*i-1] = DDE_inst.DDE_inst_data['ext_mi_checknode_in_iter'][i-1]\n",
    "    y_vec[2*i-1] = y_vec[2*i-2]\n",
    "\n",
    "    x_vec[2 * i] = x_vec[2*i-1]\n",
    "    y_vec[2 * i] = DDE_inst.DDE_inst_data['ext_mi_varnode_in_iter'][i]\n",
    "\n",
    "plt.plot(x_vec,y_vec)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DDE_inst.save_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.seterr(all='raise')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete LDPC decoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyopencl as cl\n",
    "import pyopencl.array as cl_array\n",
    "from mako.template import Template\n",
    "from pyopencl.reduction import get_sum_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discrete_LDPC_Decoder_class:\n",
    "    \"\"\"This class uses the results of Discrete Density Evolution to set up LDPC decoder that purely works on\n",
    "       lookups of integers.\n",
    "    Attributes:\n",
    "        H                           the parity check matrix of the Low-Density-Parity Check Code\n",
    "        i_max                       the number of iteration, that should be performed by the decoder\n",
    "        cardinality_Y_channel       the resolution of the continuous channel (typically a large number)\n",
    "        cardinality_T_channel       number of clusters of the channel quantizer\n",
    "        cardinality_T_decoder_ops   number of clusters used by the decoder, typically similar to cardinality_T_channel\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filename, imax_, cardinality_T_channel_,\n",
    "                 cardinality_T_decoder_ops_, Trellis_checknode_vector_a_, Trellis_varnode_vector_a_,msg_at_time_):\n",
    "        # initialize parameters\n",
    "        self.H = self.load_check_mat(filename)\n",
    "        self.imax = imax_\n",
    "\n",
    "        # Quantizer parameters\n",
    "        self.cardinality_T_channel = cardinality_T_channel_\n",
    "\n",
    "        # Discrete DE related\n",
    "        self.cardinality_T_decoder_ops = cardinality_T_decoder_ops_\n",
    "        self.Trellis_checknode_vector_a = Trellis_checknode_vector_a_.astype(int)\n",
    "        self.Trellis_varnode_vector_a = Trellis_varnode_vector_a_.astype(int)\n",
    "\n",
    "        # analyze the H matrix and set all decoder variables\n",
    "        self.degree_checknode_nr = (self.H).sum(1) # which check node has which degree?\n",
    "        self.degree_varnode_nr = (self.H).sum(0) # which variable node has which degree?\n",
    "        self.N_v = self.H.shape[1]  # How many variable nodes are present?\n",
    "        self.N_c = self.H.shape[0] # How many checknodes are present?\n",
    "\n",
    "        self.msg_at_time = msg_at_time_\n",
    "        self.map_node_connections()\n",
    "\n",
    "    def update_trellis_vectors(self,Trellis_checknode_vector_a_, Trellis_varnode_vector_a_):\n",
    "        self.Trellis_checknode_vector_a = Trellis_checknode_vector_a_.astype(int)\n",
    "        self.Trellis_varnode_vector_a = Trellis_varnode_vector_a_.astype(int)\n",
    "\n",
    "    def alistToNumpy(self, lines):\n",
    "        \"\"\"Converts a parity-check matrix in AList format to a 0/1 numpy array. The argument is a\n",
    "        list-of-lists corresponding to the lines of the AList format, already parsed to integers\n",
    "        if read from a text file.\n",
    "        The AList format is introduced on http://www.inference.phy.cam.ac.uk/mackay/codes/alist.html.\n",
    "        This method supports a \"reduced\" AList format where lines 3 and 4 (containing column and row\n",
    "        weights, respectively) and the row-based information (last part of the Alist file) are omitted.\n",
    "        Example:\n",
    "             >>> alistToNumpy([[3,2], [2, 2], [1,1,2], [2,2], [1], [2], [1,2], [1,2,3,4]])\n",
    "            array([[1, 0, 1],\n",
    "                  [0, 1, 1]])\n",
    "        \"\"\"\n",
    "\n",
    "        nCols, nRows = lines[0]\n",
    "        if len(lines[2]) == nCols and len(lines[3]) == nRows:\n",
    "            startIndex = 4\n",
    "        else:\n",
    "            startIndex = 2\n",
    "        matrix = np.zeros((nRows, nCols), dtype=np.int)\n",
    "        for col, nonzeros in enumerate(lines[startIndex:startIndex + nCols]):\n",
    "            for rowIndex in nonzeros:\n",
    "                if rowIndex != 0:\n",
    "                    matrix[rowIndex - 1, col] = 1\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    def load_check_mat(self, filename):\n",
    "        arrays = [np.array(list(map(int, line.split()))) for line in open(filename)]\n",
    "        H = self.alistToNumpy(arrays)\n",
    "        return H\n",
    "\n",
    "    def map_node_connections(self):\n",
    "        \"\"\" The overall idea of this function is to store the connections between var- and check nodes in a new structure\n",
    "        namely two vectors. This vectors are called inboxes, because the entries should be seen as memory for incoming\n",
    "        messages. Therefore it is important to track which check node output rights in which var node input and vince\n",
    "        versa. \"\"\"\n",
    "\n",
    "        self.inbox_memory_start_checknodes = np.append([0], np.cumsum(self.degree_checknode_nr[:-1]) )\n",
    "        self.inbox_memory_start_varnodes = np.append([0], np.cumsum(self.degree_varnode_nr[:-1]) )\n",
    "\n",
    "        # At first it is determined which check node delivers to which var node\n",
    "        # This information is encoded in the non-zero columns of each row\n",
    "        # non-zero return the indices in the desired way.\n",
    "\n",
    "        self.customers_checknode_nr = np.nonzero(self.H)[1]\n",
    "\n",
    "        # Now it is determined which var node delivers to which check node\n",
    "        # This information is encoded in the non-zero rows of each column\n",
    "        # non-zero return the indices in the desired way.\n",
    "\n",
    "        self.customers_varnode_nr = np.nonzero(self.H.transpose())[1]\n",
    "\n",
    "        # now we now the connections but, since one node has multiple inputs the node number is node enough.\n",
    "        # An additional offset vector needs to be defined. If one node was already connected, then the memory box is\n",
    "        # filled. Performing cumsum on the rows only allows to generate this offset vector at check nodes destinations.\n",
    "        self.offset_at_dest_checknodes = np.cumsum(self.H, 0)\n",
    "        self.offset_at_dest_checknodes = self.offset_at_dest_checknodes[np.nonzero(self.H)] - 1\n",
    "\n",
    "\n",
    "        self.target_memory_cells_checknodes = self.inbox_memory_start_varnodes[self.customers_checknode_nr] + \\\n",
    "                                              self.offset_at_dest_checknodes\n",
    "\n",
    "\n",
    "        self.offset_at_dest_varnodes = np.cumsum(self.H, 1)\n",
    "        self.offset_at_dest_varnodes = self.offset_at_dest_varnodes.transpose()[np.nonzero(self.H.transpose())] - 1\n",
    "\n",
    "\n",
    "        self.target_memory_cells_varnodes = self.inbox_memory_start_checknodes[self.customers_varnode_nr] + \\\n",
    "                                            self.offset_at_dest_varnodes\n",
    "\n",
    "\n",
    "        self.inbox_memory_checknodes = np.zeros((self.degree_checknode_nr.sum(),self.msg_at_time)).astype(int)\n",
    "        self.inbox_memory_varnodes = np.zeros((self.degree_varnode_nr.sum(),self.msg_at_time)).astype(int)\n",
    "        self.memory_channel_values = np.zeros(self.N_v)\n",
    "\n",
    "    def init_OpenCL_decoding(self,msg_at_time_, context_=False):\n",
    "        if not context_:\n",
    "            self.context = cl.create_some_context()\n",
    "        else:\n",
    "            self.context = context_\n",
    "\n",
    "        path = os.path.split(os.path.abspath(\"__file__\"))\n",
    "\n",
    "        kernelsource = open(os.path.join(path[0], \"kernels_template.cl\")).read()\n",
    "        tpl = Template(kernelsource)\n",
    "        rendered_tp = tpl.render(cn_degree=self.degree_checknode_nr[0], vn_degree=self.degree_varnode_nr[0],\n",
    "                                 msg_at_time=msg_at_time_)\n",
    "\n",
    "        self.program = cl.Program(self.context, str(rendered_tp)).build()\n",
    "\n",
    "        self.queue = cl.CommandQueue(self.context)\n",
    "        mem_pool = cl.tools.MemoryPool(cl.tools.ImmediateAllocator(self.queue, cl.mem_flags.READ_ONLY))\n",
    "        mem_pool2 = cl.tools.MemoryPool(cl.tools.ImmediateAllocator(self.queue))\n",
    "\n",
    "        #mem_pool = None\n",
    "        self.inbox_memory_start_varnodes_buffer = cl_array.to_device(self.queue,\n",
    "                                                                self.inbox_memory_start_varnodes.astype(np.int32),allocator=mem_pool)\n",
    "\n",
    "        self.inbox_memory_start_checknodes_buffer = cl_array.to_device(self.queue,\n",
    "                                                                  self.inbox_memory_start_checknodes.astype(np.int32),allocator=mem_pool)\n",
    "\n",
    "        self.degree_varnode_nr_buffer = cl_array.to_device(self.queue, self.degree_varnode_nr.astype(np.int32),allocator=mem_pool)\n",
    "\n",
    "        self.degree_checknode_nr_buffer = cl_array.to_device(self.queue, self.degree_checknode_nr.astype(np.int32),allocator=mem_pool)\n",
    "\n",
    "        self.target_memorycells_varnodes_buffer = cl_array.to_device(self.queue,\n",
    "                                                                self.target_memory_cells_varnodes.astype(np.int32),allocator=mem_pool)\n",
    "        self.target_memorycells_checknodes_buffer = cl_array.to_device(self.queue,\n",
    "                                                                  self.target_memory_cells_checknodes.astype(np.int32),allocator=mem_pool)\n",
    "\n",
    "        self.Trellis_checknode_vector_a_buffer = cl_array.to_device(self.queue, self.Trellis_checknode_vector_a.astype(np.int32),allocator=mem_pool)\n",
    "\n",
    "        self.Trellis_varnode_vector_a_buffer = cl_array.to_device(self.queue, self.Trellis_varnode_vector_a.astype(np.int32),allocator=mem_pool)\n",
    "\n",
    "        self.checknode_inbox_buffer = cl_array.empty(self.queue, self.inbox_memory_checknodes.shape, dtype=np.int32,allocator=mem_pool2)\n",
    "\n",
    "        self.varnode_inbox_buffer = cl_array.empty(self.queue, self.inbox_memory_varnodes.shape, dtype=np.int32,allocator=mem_pool2)\n",
    "\n",
    "        self.syndrom_buffer = cl_array.empty(self.queue,\n",
    "            (self.degree_checknode_nr.shape[0], self.inbox_memory_varnodes.shape[-1]), dtype=np.int32,allocator=mem_pool2)\n",
    "\n",
    "        self.krnl = get_sum_kernel(self.context, None,\n",
    "                                   dtype_in=self.varnode_inbox_buffer.dtype)  # varnode_output_buffer.dtype )\n",
    "\n",
    "\n",
    "        # define programs\n",
    "        self.send_prog = self.program.send_channel_values_to_checknode_inbox\n",
    "\n",
    "        self.first_iter_prog = self.program.checknode_update_iter0\n",
    "        self.first_iter_prog.set_scalar_arg_dtypes([None, None, None, None, None, np.int32, np.int32, None])\n",
    "\n",
    "        self.varnode_update_prog = self.program.varnode_update\n",
    "        self.varnode_update_prog.set_scalar_arg_dtypes([None, None, None, None, None, None, np.int32,\n",
    "                                                   np.int32, np.int32, None])\n",
    "\n",
    "        self.checknode_update_prog = self.program.checknode_update\n",
    "        self.checknode_update_prog.set_scalar_arg_dtypes([None, None, None, None, None, np.int32,\n",
    "                                                   np.int32, np.int32, None])\n",
    "\n",
    "        self.calc_syndrom_prog = self.program.calc_syndrome\n",
    "        self.calc_syndrom_prog.set_scalar_arg_dtypes([None, None, None, np.int32, None])\n",
    "\n",
    "        self.varoutput_prog = self.program.calc_varnode_output\n",
    "        self.varoutput_prog.set_scalar_arg_dtypes([None, None, None, None,np.int32,np.int32,np.int32, None, None ])\n",
    "\n",
    "    def decode_OpenCL(self, received_blocks,buffer_in=False,return_buffer=False):\n",
    "        # Set up OpenCL\n",
    "        if buffer_in:\n",
    "            channel_values_buffer = received_blocks\n",
    "        else:\n",
    "            channel_values_buffer = cl_array.to_device(self.queue,received_blocks.astype(np.int32))\n",
    "\n",
    "        varnode_output_buffer = cl_array.empty(self.queue, received_blocks.shape, dtype=np.int32)\n",
    "\n",
    "        self.send_prog(self.queue, received_blocks.shape, None,\n",
    "                  channel_values_buffer.data,\n",
    "                  self.inbox_memory_start_varnodes_buffer.data,\n",
    "                  self.degree_varnode_nr_buffer.data,\n",
    "                  self.target_memorycells_varnodes_buffer.data,\n",
    "                  self.checknode_inbox_buffer.data)\n",
    "        #self.queue.finish()\n",
    "\n",
    "        self.first_iter_prog(self.queue, (self.degree_checknode_nr.shape[0], received_blocks[:,np.newaxis].shape[-1]), None,\n",
    "                        self.checknode_inbox_buffer.data,\n",
    "                        self.inbox_memory_start_checknodes_buffer.data,\n",
    "                        self.degree_checknode_nr_buffer.data,\n",
    "                        self.target_memorycells_checknodes_buffer.data,\n",
    "                        self.varnode_inbox_buffer.data,\n",
    "                        self.cardinality_T_channel,\n",
    "                        self.cardinality_T_decoder_ops,\n",
    "                        self.Trellis_checknode_vector_a_buffer.data)\n",
    "\n",
    "        syndrome_zero = False\n",
    "        i_num = 1\n",
    "\n",
    "\n",
    "        while (i_num<self.imax) and (not syndrome_zero):\n",
    "\n",
    "            local_size = None #(1000, 1)\n",
    "\n",
    "            self.varnode_update_prog(self.queue, received_blocks.shape , local_size,\n",
    "                                channel_values_buffer.data,\n",
    "                                self.varnode_inbox_buffer.data,\n",
    "                                self.inbox_memory_start_varnodes_buffer.data,\n",
    "                                self.degree_varnode_nr_buffer.data,\n",
    "                                self.target_memorycells_varnodes_buffer.data,\n",
    "                                self.checknode_inbox_buffer.data,\n",
    "                                self.cardinality_T_channel,\n",
    "                                self.cardinality_T_decoder_ops,\n",
    "                                i_num-1,\n",
    "                                self.Trellis_varnode_vector_a_buffer.data\n",
    "                                )\n",
    "            #self.queue.finish()\n",
    "\n",
    "            self.checknode_update_prog(self.queue, (self.degree_checknode_nr.shape[0], received_blocks[:,np.newaxis].shape[-1]), None,\n",
    "                                   self.checknode_inbox_buffer.data,\n",
    "                                   self.inbox_memory_start_checknodes_buffer.data,\n",
    "                                   self.degree_checknode_nr_buffer.data,\n",
    "                                   self.target_memorycells_checknodes_buffer.data,\n",
    "                                   self.varnode_inbox_buffer.data,\n",
    "                                   self.cardinality_T_channel,\n",
    "                                   self.cardinality_T_decoder_ops,\n",
    "                                   i_num-1,\n",
    "                                   self.Trellis_checknode_vector_a_buffer.data)\n",
    "\n",
    "            #self.queue.finish()\n",
    "\n",
    "            self.calc_syndrom_prog(self.queue, (self.degree_checknode_nr.shape[0], received_blocks[:,np.newaxis].shape[-1]), None,\n",
    "                                      self.checknode_inbox_buffer.data,\n",
    "                                      self.inbox_memory_start_checknodes_buffer.data,\n",
    "                                      self.degree_checknode_nr_buffer.data,\n",
    "                                      self.cardinality_T_decoder_ops,\n",
    "                                      self.syndrom_buffer.data)\n",
    "\n",
    "            #self.queue.finish()\n",
    "\n",
    "            if cl_array.sum(self.syndrom_buffer).get() == 0:\n",
    "                 syndrome_zero =True\n",
    "\n",
    "            i_num += 1\n",
    "\n",
    "\n",
    "        self.varoutput_prog(self.queue, received_blocks.shape , None,\n",
    "                            channel_values_buffer.data,\n",
    "                            self.varnode_inbox_buffer.data,\n",
    "                            self.inbox_memory_start_varnodes_buffer.data,\n",
    "                            self.degree_varnode_nr_buffer.data,\n",
    "                            self.cardinality_T_channel,\n",
    "                            self.cardinality_T_decoder_ops,\n",
    "                            i_num - 1,\n",
    "                            self.Trellis_varnode_vector_a_buffer.data,\n",
    "                            varnode_output_buffer.data)\n",
    "        self.queue.finish()\n",
    "        if return_buffer:\n",
    "            return varnode_output_buffer\n",
    "        else:\n",
    "            pass\n",
    "            output_values = varnode_output_buffer.get()\n",
    "            return output_values\n",
    "\n",
    "    def return_errors_all_zero(self, varnode_output_buffer):\n",
    "\n",
    "        errors = self.krnl((varnode_output_buffer.__lt__(int(self.cardinality_T_decoder_ops / 2)).astype(np.int32))).get()\n",
    "        return errors\n",
    "\n",
    "    def discrete_cn_operation(self,vec_y_c,iter_):\n",
    "        self.d_c = self.degree_checknode_nr[0]\n",
    "\n",
    "        if (iter_ == 0):\n",
    "            t_0_c = self.Trellis_checknode_vector_a[vec_y_c[:, 0]*self.cardinality_T_channel + vec_y_c[:, 1]]\n",
    "\n",
    "            t_l_m_1_c = t_0_c\n",
    "\n",
    "            for l in range(self.d_c - 3):\n",
    "                t_l_c = self.Trellis_checknode_vector_a[t_l_m_1_c * self.cardinality_T_decoder_ops +\n",
    "                                                        vec_y_c[:, l + 2] + self.cardinality_T_channel ** 2\n",
    "                                                        + l * self.cardinality_T_decoder_ops * self.cardinality_T_channel]\n",
    "                t_l_m_1_c = t_l_c\n",
    "\n",
    "        else:\n",
    "            offset_iteration_0 = 1 * (self.d_c - 3) * self.cardinality_T_channel * self.cardinality_T_decoder_ops + \\\n",
    "                                 1 * self.cardinality_T_channel ** 2\n",
    "            add_offset_iteration_iter = (iter_ - 1) * (self.d_c - 2) * self.cardinality_T_decoder_ops ** 2\n",
    "\n",
    "            t_0_c = self.Trellis_checknode_vector_a[vec_y_c[:, 0]*self.cardinality_T_decoder_ops +\n",
    "                                                   vec_y_c[:, 1] + offset_iteration_0 + add_offset_iteration_iter]\n",
    "\n",
    "            t_l_m_1_c = t_0_c\n",
    "\n",
    "            for l in range(self.d_c - 3):\n",
    "                t_l_c = self.Trellis_checknode_vector_a[t_l_m_1_c * self.cardinality_T_decoder_ops +\n",
    "                                                        vec_y_c[:, l + 2] +\n",
    "                                                        (l+1) * self.cardinality_T_decoder_ops ** 2 +\n",
    "                                                        offset_iteration_0 + add_offset_iteration_iter]\n",
    "                t_l_m_1_c = t_l_c\n",
    "\n",
    "\n",
    "        node_output_msg = t_l_m_1_c\n",
    "        return node_output_msg\n",
    "\n",
    "    def discrete_vn_operation(self, vec_y_v, iter_):\n",
    "        self.d_v = self.degree_varnode_nr[0]\n",
    "\n",
    "        offset_iteration_iter = (1 * self.cardinality_T_channel * self.cardinality_T_decoder_ops + (\n",
    "            self.d_v - 1) * self.cardinality_T_decoder_ops ** 2) * (iter_)\n",
    "\n",
    "        t_0_v = self.Trellis_varnode_vector_a[vec_y_v[:, 0]*self.cardinality_T_decoder_ops +\n",
    "                                              vec_y_v[:, 1] + offset_iteration_iter]\n",
    "\n",
    "        t_l_m_1_v = t_0_v\n",
    "        for l in range(vec_y_v.shape[1]- 2):\n",
    "            t_l_v = self.Trellis_varnode_vector_a[t_l_m_1_v * self.cardinality_T_decoder_ops + vec_y_v[:, l + 2] +\n",
    "                                                 l * self.cardinality_T_decoder_ops ** 2 +\n",
    "                                                 offset_iteration_iter +\n",
    "                                                 1 * self.cardinality_T_channel * self.cardinality_T_decoder_ops]\n",
    "            t_l_m_1_v = t_l_v\n",
    "\n",
    "        node_output_msg = t_l_m_1_v\n",
    "        return node_output_msg\n",
    "\n",
    "    def decode_on_host(self,channel_values_):\n",
    "\n",
    "        self.memory_channel_values = channel_values_\n",
    "\n",
    "        channel_val_mat = np.kron(self.memory_channel_values[:,np.newaxis], np.ones( (self.degree_varnode_nr[0],1) )).astype(int)\n",
    "\n",
    "        start_idx_var = self.inbox_memory_start_varnodes\n",
    "        ind_mat_var = start_idx_var[:,np.newaxis] + np.arange(self.degree_varnode_nr[0])\n",
    "\n",
    "\n",
    "        self.inbox_memory_checknodes[:,0][self.target_memory_cells_varnodes[ind_mat_var]] = channel_val_mat\n",
    "\n",
    "        start_idx_check = self.inbox_memory_start_checknodes\n",
    "        index_mat_check = start_idx_check[:,np.newaxis] + np.arange(self.degree_checknode_nr[0])\n",
    "\n",
    "\n",
    "        customers_check = np.reshape(self.target_memory_cells_checknodes[index_mat_check], (-1,1))[:,0]\n",
    "        customers_var = np.reshape(self.target_memory_cells_varnodes[ind_mat_var],(-1,1))\n",
    "\n",
    "        for iter in range(self.imax):\n",
    "            all_messages = self.inbox_memory_checknodes[index_mat_check]\n",
    "\n",
    "            m = np.kron(np.arange(self.degree_checknode_nr[0])[:,np.newaxis],np.ones(self.degree_checknode_nr[0])) #'*ones(1,self.degree_checknode_nr(1));\n",
    "            reduced = all_messages[:, m.transpose()[ (1 - np.eye(self.degree_checknode_nr[0])).astype(bool) ].astype(int)]\n",
    "            reduced = np.reshape(reduced,(-1,self.degree_checknode_nr[0]-1))\n",
    "\n",
    "\n",
    "            self.inbox_memory_varnodes[customers_check, 0] = self.discrete_cn_operation(reduced, iter)\n",
    "\n",
    "\n",
    "            all_messages = self.inbox_memory_varnodes[ind_mat_var]\n",
    "\n",
    "            m = np.kron(np.arange(self.degree_varnode_nr[0])[:, np.newaxis], np.ones(self.degree_varnode_nr[0]))\n",
    "\n",
    "            reduced = all_messages[:, m.transpose()[(1 - np.eye(self.degree_varnode_nr[0])).astype(bool)].astype(int)]\n",
    "            reduced = np.reshape(reduced, (-1, self.degree_varnode_nr[0] - 1))\n",
    "\n",
    "            self.inbox_memory_checknodes[:,0][customers_var] = self.discrete_vn_operation(np.hstack((channel_val_mat, reduced)),iter)\n",
    "\n",
    "        all_messages = self.inbox_memory_varnodes[ind_mat_var]\n",
    "\n",
    "        output_vector = self.discrete_vn_operation(np.hstack((self.memory_channel_values[:,np.newaxis], all_messages[:,:,0])), self.imax-1)\n",
    "\n",
    "        return output_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDPC Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDPCEncoder:\n",
    "    \"\"\"This class implements an LDPC encoder. The constructor takes the path to a saved Parity Check Matrix as input. The\n",
    "    file should be in the alist Format. Similar to the LDPCencoder from the Matlab communication toolbox the  last\n",
    "    NK columns in the parity check matrix must be an invertible matrix in GF(2).\n",
    "    This is because the encoding is done only based on parity check matrix, by evaluating a_k' = inv(H_k)*H_l*a_L.\n",
    "    Input X must be a numeric or logical column vector with length equal K. The length of the encoded data output\n",
    "    vector, Y, is N. It is a solution to the parity-check equation, with the first K bits equal to the input, X.\"\"\"\n",
    "\n",
    "    def __init__(self, filename, alist_file = True):\n",
    "        #if alist_file:\n",
    "        #    self.H = self.load_check_mat(filename)\n",
    "        #else:\n",
    "        #    self.H = np.load(filename)\n",
    "\n",
    "        # if sp.issparse(self.H):\n",
    "        #    self.H = (self.H).toarray()\n",
    "        # self.H_sparse = sp.csr_matrix(self.H)\n",
    "        # self.setParityCheckMatrix(self.H)\n",
    "\n",
    "        self.H_sparse = self.load_check_mat(filename)\n",
    "        self.setParityCheckMatrix(self.H_sparse)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def alistToNumpy(self,lines):\n",
    "        \"\"\"Converts a parity-check matrix in AList format to a 0/1 numpy array. The argument is a\n",
    "       list-of-lists corresponding to the lines of the AList format, already parsed to integers\n",
    "        if read from a text file.\n",
    "        The AList format is introduced on http://www.inference.phy.cam.ac.uk/mackay/codes/alist.html.\n",
    "        This method supports a \"reduced\" AList format where lines 3 and 4 (containing column and row\n",
    "        weights, respectively) and the row-based information (last part of the Alist file) are omitted.\n",
    "        Example:\n",
    "             >>> alistToNumpy([[3,2], [2, 2], [1,1,2], [2,2], [1], [2], [1,2], [1,2,3,4]])\n",
    "            array([[1, 0, 1],\n",
    "                  [0, 1, 1]])\n",
    "        \"\"\"\n",
    "\n",
    "        nCols, nRows = lines[0]\n",
    "        if len(lines[2]) == nCols and len(lines[3]) == nRows:\n",
    "            startIndex = 4\n",
    "        else:\n",
    "            startIndex = 2\n",
    "        matrix = np.zeros((nRows, nCols), dtype=np.int)\n",
    "        for col, nonzeros in enumerate(lines[startIndex:startIndex + nCols]):\n",
    "            for rowIndex in nonzeros:\n",
    "                if rowIndex != 0:\n",
    "                    matrix[rowIndex - 1, col] = 1\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    def load_sparse_csr(self,filename):\n",
    "        loader = np.load(filename)\n",
    "        return sp.csr_matrix((loader['data'], loader['indices'], loader['indptr']),\n",
    "                              shape=loader['shape'])\n",
    "\n",
    "    def load_check_mat(self, filename):\n",
    "        if filename.endswith('.npy') or filename.endswith('.npz'):\n",
    "            if filename.endswith('.npy'):\n",
    "                H = np.load(filename)\n",
    "                H = sp.csr_matrix(H)\n",
    "            else:\n",
    "                H = self.load_sparse_csr(filename)\n",
    "\n",
    "        else:\n",
    "            arrays = [np.array(list(map(int, line.split()))) for line in open(filename)]\n",
    "            H = self.alistToNumpy(arrays)\n",
    "            H = sp.csr_matrix(H)\n",
    "\n",
    "        return H\n",
    "\n",
    "    def encode(self, X):\n",
    "\n",
    "        EncodingMethod = self.EncodingMethod.copy()\n",
    "\n",
    "        if self.RowOrder[0] >= 0:\n",
    "            # only if the last (N-K) columns of H are not triangular or if they are lower/upper triangular along the\n",
    "            # antidiagonal\n",
    "            # Todo\n",
    "            pass\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # compute matrix product between first K_columns of H and information bits.\n",
    "        result = self.GF2MatrixMul(X, np.zeros(self.NumParityBits, dtype=int), self.NumInfoBits, self.MatrixA_RowIndices,\n",
    "                                   self.MatrixA_RowStartLoc, self.MatrixA_ColumnSum, 1)\n",
    "\n",
    "        # need to perform another substitution if last (N-K) columns are not triangular\n",
    "        if EncodingMethod == 0:\n",
    "            # forward substitution for lower triangular matrix obtained from factorization in GF(2)\n",
    "            result = self.GF2MatrixMul(result, result, self.NumParityBits, self.MatrixL_RowIndices,\n",
    "                                       self.MatrixL_RowStartLoc, self.MatrixL_ColumnSum, 1)\n",
    "            # now we need to perform backward substitution since B will be upper triangular\n",
    "            EncodingMethod = -1\n",
    "\n",
    "        if self.RowOrder[0] >= 0:\n",
    "            #first version loop\n",
    "            #for counter in range(self.NumParityBits):\n",
    "            #    result[counter] = result[self.RowOrder[counter]]\n",
    "            # second option\n",
    "            result = result[self.RowOrder]\n",
    "\n",
    "        # Solve for the Parity Check Bits.\n",
    "        # Common step for all shapes.\n",
    "        parity_check_bits = self.GF2MatrixMul(result, result, self.NumParityBits, self.MatrixB_RowIndices,\n",
    "                                              self.MatrixB_RowStartLoc, self.MatrixB_ColumnSum, EncodingMethod)\n",
    "\n",
    "        codeword = np.append(X, parity_check_bits)\n",
    "        return codeword\n",
    "\n",
    "    def encode_c(self, X):\n",
    "        EncodingMethod = self.EncodingMethod\n",
    "\n",
    "        if self.RowOrder[0] >= 0:\n",
    "            # only if the last (N-K) coloums of H are not triangula or if they are lower/upper triangular along the\n",
    "            # antidiagonal\n",
    "            # Todo\n",
    "            pass\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # compute matrix product between first K_columns of H and information bits.\n",
    "        result = GF2MatrixMul_c.GF2MatrixMul_c(X.astype(np.int32).copy(), np.zeros(self.NumParityBits, dtype=np.int32),\n",
    "                                               self.NumInfoBits, self.MatrixA_RowIndices,self.MatrixA_RowStartLoc,\n",
    "                                               self.MatrixA_ColumnSum, 1)\n",
    "\n",
    "        # need to perform another substitution if last (N-K) columns are not triangular\n",
    "        if EncodingMethod == 0:\n",
    "            # forward substitution for lower triangular matrix obtained from factorization in GF(2)\n",
    "            result = GF2MatrixMul_c.GF2MatrixMul_c(result, result, self.NumParityBits, self.MatrixL_RowIndices,\n",
    "                                       self.MatrixL_RowStartLoc, self.MatrixL_ColumnSum, 1)\n",
    "            # now we need to perform backward substitution since B will be upper triangular\n",
    "            EncodingMethod = -1\n",
    "\n",
    "        if self.RowOrder[0] >= 0:\n",
    "            #first version loop\n",
    "            #for counter in range(self.NumParityBits):\n",
    "            #    result[counter] = result[self.RowOrder[counter]]\n",
    "            # second option\n",
    "            result = result[self.RowOrder]\n",
    "\n",
    "        # Solve for the Parity Check Bits.\n",
    "        # Common step for all shapes.\n",
    "        parity_check_bits = GF2MatrixMul_c.GF2MatrixMul_c(result, result, self.NumParityBits, self.MatrixB_RowIndices,\n",
    "                                              self.MatrixB_RowStartLoc, self.MatrixB_ColumnSum, EncodingMethod)\n",
    "\n",
    "        codeword = np.append(X, parity_check_bits)\n",
    "        return codeword\n",
    "\n",
    "    def GF2MatrixMul(self, source, dest, srclen ,RowIndices, RowLoc, ColumnSum, direction):\n",
    "        \"\"\" example: \n",
    "            source: InformationBits\n",
    "            dest: MatrixProductbuffer (return value)\n",
    "            srclen: NumInfoBits\n",
    "            RowIndices: A_RowIndices (of matrix A which is the H(:,1:K)\n",
    "            RowLoc: A_RowStartLoc\n",
    "            CoulumnSum\n",
    "            direction: 1 or -1 forward backward substitution\n",
    "            \"\"\"\n",
    "\n",
    "        if direction == 1:\n",
    "            columnindex = 0 # Start from the first column for forward substitution\n",
    "        else:\n",
    "            columnindex = srclen - 1 # Start from the last column for backward substitution\n",
    "\n",
    "        \n",
    "        for col_counter in range(srclen):\n",
    "            if not source[columnindex] == 0:\n",
    "                for row_counter in range(ColumnSum[columnindex]):\n",
    "                    rowindex = RowIndices[RowLoc[columnindex] + row_counter]\n",
    "                    dest[rowindex] = 1 - dest[rowindex]\n",
    "\n",
    "            columnindex += direction\n",
    "\n",
    "\n",
    "        return dest\n",
    "            \n",
    "\n",
    "    def setParityCheckMatrix(self,H):\n",
    "        params = self.getLDPCEncoderParamters(H)\n",
    "        self.storedParityCheckMatrix = H\n",
    "\n",
    "    def getLDPCEncoderParamters(self,H):\n",
    "        self.N = H.shape[1]\n",
    "        self.K = self.N -H.shape[0]\n",
    "\n",
    "        #extract last (N-K) columns of parity check matrix\n",
    "        last_Part = H[:,self.K:]\n",
    "\n",
    "        # check if last_Part is triangular\n",
    "        shape = self.isfulldiagtriangular(last_Part)\n",
    "\n",
    "        if shape == 1:\n",
    "            algo = 'Forward Substitution'\n",
    "            rowOrder = np.array([-1])                   # Don't need to reverse the order\n",
    "        elif shape == -1:\n",
    "            algo = 'Backward Substitution'\n",
    "            rowOrder = np.array([-1])                   # Don't need to reverse the order\n",
    "        else:\n",
    "            # Reverse the order of rows in last_Part, but keep lastPart, since if PB is not triangular\n",
    "            # we need to factorize it in GF(2)\n",
    "            Reversed_last = last_Part[::-1,:].copy()\n",
    "            rev_shape = self.isfulldiagtriangular(Reversed_last)\n",
    "            if rev_shape == 1:\n",
    "                algo = 'Forward Substitution'\n",
    "                rowOrder = np.arange((self.N-self.K))[::-1]\n",
    "                last_Part =  Reversed_last\n",
    "            elif rev_shape == -1:\n",
    "                algo = 'Backward Substitution'\n",
    "                rowOrder = np.arange((self.N-self.K))[::-1]\n",
    "                last_Part = Reversed_last\n",
    "            else:\n",
    "                algo = 'Matrix Inverse'\n",
    "\n",
    "        # now we preallocate variable for the encode function\n",
    "        self.MatrixL_RowIndices = np.int32(0)\n",
    "        self.MatrixL_ColumnSum = np.int32(0)\n",
    "        self.MatrixL_RowStartLoc = np.int32(0)\n",
    "\n",
    "        if algo == 'Forward Substitution':\n",
    "            self.EncodingMethod = np.int8(1)\n",
    "            #P = np.tril(last_Part, -1) # remove diagonal\n",
    "            P = sp.tril(last_Part, -1) # remove diagonal\n",
    "        elif algo == 'Backward Substitution':\n",
    "            self.EncodingMethod = np.int8(1)\n",
    "            #P = np.triu(last_Part, 1) # remove diagonal\n",
    "            P = sp.triu(last_Part, 1) # remove diagonal\n",
    "        else:\n",
    "            # algo is 'Matrix Inverse' so we need to work a bit. So we factorize in GF(2) first.\n",
    "            PL, last_Part, rowOrder, invertible = self.gf2factorize(last_Part.toarray())\n",
    "\n",
    "            if not invertible:\n",
    "                print('Not invertible Matrix')\n",
    "            self.EncodingMethod = np.int8(0)\n",
    "            #self.MatrixL_RowIndices, self.MatrixL_RowStartLoc, self.MatrixL_ColumnSum = \\\n",
    "            #    self.ConvertMatrixFormat(np.tril(PL, -1))\n",
    "\n",
    "            self.MatrixL_RowIndices, self.MatrixL_RowStartLoc, self.MatrixL_ColumnSum = \\\n",
    "                self.ConvertMatrixFormat(sp.tril(PL, -1))\n",
    "\n",
    "            last_Part = last_Part[rowOrder, :]\n",
    "            #P = np.triu(last_Part, 1)\n",
    "            P = sp.triu(last_Part, 1)\n",
    "\n",
    "        # Update all internal data structures for the encoding\n",
    "        self.RowOrder = np.int32(rowOrder)\n",
    "\n",
    "        self.MatrixA_RowIndices, self.MatrixA_RowStartLoc, self.MatrixA_ColumnSum = self.ConvertMatrixFormat(H[:, :self.K])\n",
    "        self.MatrixB_RowIndices, self.MatrixB_RowStartLoc, self.MatrixB_ColumnSum = self.ConvertMatrixFormat(P)\n",
    "\n",
    "        # Update all external properties.\n",
    "        self.NumInfoBits = self.K\n",
    "        self.NumParityBits = self.N - self.K\n",
    "        self.BlockLength = self.N\n",
    "        self.EncodingAlgorithm = algo\n",
    "\n",
    "    def ConvertMatrixFormat(self, X):\n",
    "        \"\"\"Create an alternative representation of zero-one matrix\"\"\"\n",
    "\n",
    "        # j, i = np.nonzero(np.transpose(X.toarray()))\n",
    "        # RowIndices = np.int32(i)\n",
    "        # ColumnSum = np.int32((X.toarray()).sum(0))\n",
    "        # # For each row find the corresponding row indices start in RowIndicies.\n",
    "        # CumulativeSum = np.cumsum(np.double(ColumnSum))\n",
    "        # RowStartLoc = np.int32(np.append([0], CumulativeSum[:-1]))\n",
    "\n",
    "        RowIndices = ((X.tocsc()).indices).astype(np.int32)\n",
    "        RowStartLoc = np.int32((X.tocsc()).indptr[:-1])\n",
    "        ColumnSum = np.int32((X.tocsc().sum(0)).A[0,:])\n",
    "\n",
    "        return RowIndices, RowStartLoc, ColumnSum\n",
    "\n",
    "    def gf2factorize(self,X):\n",
    "        \"\"\"This function factorizes a square matrix in GF(2) using Gaussian elimination.\n",
    "        X= A * B using modulo 2 arithmetic.\n",
    "        X may be sparse.\n",
    "        A and B will be sparse\n",
    "\n",
    "        A is always lower triangular. If X is invertible in GF(2), then B(chosen_pivot,:) is upper triangular and\n",
    "        invertible.\n",
    "        \"\"\"\n",
    "\n",
    "        n = X.shape[0]\n",
    "        if not n == X.shape[1]:\n",
    "            print(\"error non square matrix\")\n",
    "\n",
    "        Y1 = np.eye(n,n,0,bool)\n",
    "        Y2 = np.zeros([n,n]).astype(bool)\n",
    "        Y2[np.nonzero(X)] = 1\n",
    "        chosen_pivots = np.zeros(n).astype(int)\n",
    "        invertible = True\n",
    "\n",
    "        for col in range(n):\n",
    "            candidate_rows = Y2[:, col].copy()\n",
    "\n",
    "            candidate_rows[chosen_pivots[:col]] = 0 # never use a chosen pivot\n",
    "            candidate_rows = np.nonzero(candidate_rows)[0]\n",
    "\n",
    "            if candidate_rows.size ==0:\n",
    "                invertible = False # not invertible\n",
    "                break\n",
    "            else:\n",
    "                pivot = candidate_rows[0] # chose first candidate as pivot\n",
    "                chosen_pivots[col] = pivot # record pivot\n",
    "                # find all nonzero elements in pivot row and xor with corresponding other candidate rows\n",
    "                columnind = np.nonzero(Y2[pivot, :])\n",
    "\n",
    "                # subtraction step. but is NOT in GF2\n",
    "\n",
    "\n",
    "                Y2[candidate_rows[1:, np.newaxis], columnind] = \\\n",
    "                    np.logical_not(Y2[candidate_rows[1:, np.newaxis], columnind])\n",
    "\n",
    "\n",
    "                Y1[candidate_rows[1:], pivot] = 1\n",
    "\n",
    "        A = sp.csr_matrix(Y1)\n",
    "        B = sp.csr_matrix(Y2)\n",
    "        #A = Y1\n",
    "        #B = Y2\n",
    "\n",
    "        #if not invertible return empty pivot\n",
    "        if not invertible:\n",
    "            chosen_pivots = np.zeros(n).astype(int)\n",
    "\n",
    "        return A, B, chosen_pivots, invertible\n",
    "\n",
    "    def isfulldiagtriangular(self,X):\n",
    "        \"\"\"X must be a square logical matrix.\n",
    "        shape = 1 if X is ower triangular and has a full diagonal\n",
    "        shape = -1 if X is upper triangular and has a full diagonal\n",
    "        shape = 0\"\"\"\n",
    "\n",
    "        N = X.shape[0]\n",
    "        NumNonZeros = (X != 0).sum()\n",
    "        #if not np.all(np.diagonal(X)):\n",
    "        if not np.all(X.diagonal()):\n",
    "            shape = 0\n",
    "        else:\n",
    "            #NumNonzerosInLowerPart = (np.tril(X) != 0).sum()\n",
    "            NumNonzerosInLowerPart = (sp.tril(X) != 0).sum()\n",
    "            if NumNonzerosInLowerPart == NumNonZeros:\n",
    "                shape = 1 # X is lower triangular\n",
    "            elif  NumNonzerosInLowerPart == N:\n",
    "                shape = -1\n",
    "            else:\n",
    "                shape = 0\n",
    "        return shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.seterr(all='raise')\n",
    "\n",
    "# Load stored data\n",
    "filepath =\"8000.4000.3.483.alist\"\n",
    "\n",
    "decoder_name = 'decoder_config_EbN0_gen_1.2_16.pkl'\n",
    "pkl_file = open(decoder_name, 'rb')\n",
    "generated_decoder = pickle.load(pkl_file)\n",
    "\n",
    "Trellis_checknodevector_a = generated_decoder['Trellis_checknodevector_a']\n",
    "Trellis_varnodevector_a = generated_decoder['Trellis_varnodevector_a']\n",
    "\n",
    "# Encode Codeword\n",
    "\n",
    "# Human choice\n",
    "AD_max_abs = 3\n",
    "cardinality_Y_channel = 2000\n",
    "cardinality_T_channel = 16\n",
    "cardinality_T_decoder_ops = generated_decoder['cardinality_T_decoder_ops']\n",
    "msg_at_time = 100\n",
    "min_errors = 4000\n",
    "\n",
    "imax = generated_decoder['imax']\n",
    "N_var = 8000\n",
    "\n",
    "#sets the start EbN0_dB value\n",
    "EbN0_dB_max_value = 1.9\n",
    "\n",
    "#simulation runs until this BER is achieved\n",
    "target_error_rate=1e-6\n",
    "BER_go_on_in_smaller_steps=1e-6\n",
    "\n",
    "#in steps of size..\n",
    "EbN0_dB_normal_stepwidth=0.1\n",
    "EbN0_dB_small_stepwidth=0.1\n",
    "\n",
    "# start EbN0 simulation\n",
    "EbN0_dB = 0\n",
    "EbN0_dB_ind = 0\n",
    "BER_vector = np.array([0.])\n",
    "EbN0_dB_vector = np.array([EbN0_dB])\n",
    "ready = False\n",
    "NR_BLOCKS_PER_CONTROL_MSG = 100\n",
    "\n",
    "transi = LDPC_BPSK_Transmitter (filepath, msg_at_time)\n",
    "decodi = Discrete_LDPC_Decoder_class(filepath, imax, cardinality_T_channel, cardinality_T_decoder_ops, Trellis_checknodevector_a,\n",
    "                      Trellis_varnodevector_a,msg_at_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while not ready:\n",
    "    EbN0_dB_ind += EbN0_dB_ind\n",
    "    EbN0_dB = EbN0_dB_vector[-1]\n",
    "\n",
    "    sigma_n2 = 10**(-EbN0_dB/10) / (2*transi.R_c)\n",
    "\n",
    "    #chani = AWGN_channel(sigma_n2)\n",
    "\n",
    "    quanti = AWGN_Channel_Quantizer(sigma_n2, AD_max_abs, cardinality_T_channel, cardinality_Y_channel)\n",
    "    quanti.init_OpenCL_quanti(N_var,msg_at_time,return_buffer_only=True)\n",
    "    decodi.init_OpenCL_decoding(msg_at_time,quanti.context)\n",
    "\n",
    "\n",
    "    errors = 0\n",
    "    transmitted_blocks = 0\n",
    "    # transmit\n",
    "    start = time.time()\n",
    "    while errors < min_errors:\n",
    "\n",
    "        rec_data_quantized = quanti.quantize_direct_OpenCL(N_var, msg_at_time)\n",
    "        decoded_mat = decodi.decode_OpenCL(rec_data_quantized,buffer_in=True,return_buffer=True)\n",
    "\n",
    "        errors += decodi.return_errors_all_zero(decoded_mat)\n",
    "        transmitted_blocks += + msg_at_time\n",
    "\n",
    "\n",
    "        if np.mod(transmitted_blocks, NR_BLOCKS_PER_CONTROL_MSG) == 0:\n",
    "            time_so_far = time.time()-start\n",
    "            time_per_error = (time_so_far / (errors+1)) #+1 to avoid devide by 0 errors\n",
    "            estim_minutes_left = ((min_errors * time_per_error) - time_so_far) / 60\n",
    "\n",
    "            print('EbN0_dB=', EbN0_dB, ', '\n",
    "                  'errors=', errors,\n",
    "                  ' elapsed time this run=', time_so_far,\n",
    "                  ' BER_estimate=','{:.2e}'.format( (errors / (transmitted_blocks * N_var))),\n",
    "                  ' datarate_Bps =', '{:.2e}'.format(  (transmitted_blocks * N_var) / time_so_far),\n",
    "                  ' estim_minutes_left=',estim_minutes_left)\n",
    "\n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    BER_vector[-1] = errors / (transmitted_blocks * N_var)\n",
    "    spent = end-start\n",
    "    datarate_Bps = (transmitted_blocks * N_var) / spent\n",
    "    print(EbN0_dB_vector[-1], '{:.2e}'.format(BER_vector[-1]), ' Bitrate:','{:.2e}'.format(datarate_Bps) )\n",
    "\n",
    "    if (BER_vector[-1] > target_error_rate) and (EbN0_dB < EbN0_dB_max_value):\n",
    "        if BER_vector[-1] < BER_go_on_in_smaller_steps:\n",
    "            EbN0_dB_vector = np.append(EbN0_dB_vector, EbN0_dB_vector[-1] + EbN0_dB_small_stepwidth)\n",
    "        else:\n",
    "            EbN0_dB_vector = np.append(EbN0_dB_vector, EbN0_dB_vector[-1] + EbN0_dB_normal_stepwidth)\n",
    "\n",
    "        BER_vector = np.append(BER_vector, 0)\n",
    "    else:\n",
    "        ready = True\n",
    "\n",
    "timestr = time.strftime(\"%y%m%d-%H%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.splitext(decoder_name)[0].replace('.','').replace('_config','')+'_'+timestr\n",
    "pathname = os.path.join('BER_Results', filename.replace('.', ''))\n",
    "os.makedirs(os.path.dirname(os.path.join(pathname,' ')), exist_ok=True)\n",
    "\n",
    "\n",
    "#Plot\n",
    "plt.semilogy(EbN0_dB_vector,BER_vector)\n",
    "plt.xlabel('Eb/N0')\n",
    "plt.ylabel('Bit Error Rate ')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "np.savez(os.path.join(pathname,'BER_results'), EbN0_dB_vector=EbN0_dB_vector, BER_vector=BER_vector)\n",
    "\n",
    "plt.savefig(os.path.join(pathname,'BER_figure.pgf'))\n",
    "plt.savefig(os.path.join(pathname,'BER_figure.pdf'))\n",
    "\n",
    "res_dict = {\"EbN0_dB_vector\" : EbN0_dB_vector, \"BER_vector\":BER_vector\n",
    "    , \"decoder_name\":decoder_name}\n",
    "\n",
    "sio.savemat(os.path.join(pathname,'BER_results.mat'),res_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "55c96426c209bb0b838a038f619d6b23b91504976d406b82cd1da81686961396"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
